{
  "keyword": "language_learning",
  "matchTopics": [{
    "topic": [{
      "word": "language_acquisition",
      "value": 0.20751342177391052
    }, {
      "word": "language_learning",
      "value": 0.1520572453737259
    }, {
      "word": "child_language",
      "value": 0.06746741384267807
    }, {
      "word": "learning_algorithm",
      "value": 0.05954510718584061
    }, {
      "word": "learning_process",
      "value": 0.050345003604888916
    }, {
      "word": "word_learning",
      "value": 0.045489393174648285
    }, {
      "word": "negative_evidence",
      "value": 0.03526705875992775
    }, {
      "word": "child-directed_speech",
      "value": 0.035011500120162964
    }, {
      "word": "language_learner",
      "value": 0.035011500120162964
    }, {
      "word": "language_learners",
      "value": 0.03475594148039818
    }, {
      "word": "learning_theory",
      "value": 0.031178124248981476
    }, {
      "word": "computational_models",
      "value": 0.02913365699350834
    }, {
      "word": "learning_system",
      "value": 0.027344748377799988
    }, {
      "word": "learning_language",
      "value": 0.027089189738035202
    }, {
      "word": "learning_model",
      "value": 0.026833631098270416
    }, {
      "word": "children_learn",
      "value": 0.023766931146383286
    }, {
      "word": "computational_model",
      "value": 0.023766931146383286
    }, {
      "word": "learning_task",
      "value": 0.022744696587324142
    }, {
      "word": "childes_database",
      "value": 0.022489137947559357
    }, {
      "word": "language_development",
      "value": 0.02172246389091015
    }, {
      "word": "young_children",
      "value": 0.021466905251145363
    }],
    "documents": [{
      "author": "Yang, Charles D.",
      "title": "A Selectionist Theory Of Language Acquisition",
      "id": "P99-1055",
      "abstractText": "This paper argues that developmental patterns in child language be taken seriously in computational models of language acquisition, and proposes a formal theory that meets this criterion. We first present developmental facts that are problematic for statistical learning approaches which assume no prior knowledge of grammar, and for traditional learnabil-ity models which assume the learner moves from one UG-defined grammar to another. In contrast, we view language acquisition as a population of grammars associated with \"weights\", that compete in a Darwinian selectionist process. Selection is made possible by the variational properties of individual grammars; specifically, their differential compatibility with the primary linguistic data in the environment. In addition to a convergence proof, we present empirical evidence in child language development, that a learner is best modeled as multiple grammars in coexistence and competition. ",
      "year": "1999",
      "pedagogicalRole": null,
      "pageRankScore": 5.2213963E-5,
      "relevanceScore": 0.5861056845876617,
      "authorScore": 0.0,
      "relevantTopics": [{
        "topicName": "Language acquisition",
        "strength": 0.5861056845876617
      }, {
        "topicName": "Rule-based parsing",
        "strength": 0.06359550263544957
      }, {
        "topicName": "Natural language & lexical semantics",
        "strength": 0.04608735816925561
      }, {
        "topicName": "Computational linguistics",
        "strength": 0.044094375696947075
      }, {
        "topicName": "Noun phrases",
        "strength": 0.039931142214077744
      }, {
        "topicName": "Genetic algorithms & anaphora resolution",
        "strength": 0.024717169525345285
      }, {
        "topicName": "Natural languages & computational linguistics",
        "strength": 0.022254646025394828
      }, {
        "topicName": "Confusion sets",
        "strength": 0.019674379611831997
      }, {
        "topicName": "Parallel corpora for machine translation",
        "strength": 0.017701452999750643
      }, {
        "topicName": "Generative models & model selection",
        "strength": 0.01722359623561319
      }]
    }, {
      "author": "Buttery, Paula",
      "title": "A Quantitative Evaluation Of Naturalistic Models Of Language Acquisition; The Efficiency Of The Triggering Learning Algorithm Compared To A Categorial Grammar Learner",
      "id": "W04-1301",
      "abstractText": "Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efficiency against more reliable convergence to the target grammar. ",
      "year": "2004",
      "pedagogicalRole": null,
      "pageRankScore": 3.229716E-5,
      "relevanceScore": 0.5381193166905334,
      "authorScore": 0.0,
      "relevantTopics": [{
        "topicName": "Language acquisition",
        "strength": 0.5381193166905334
      }, {
        "topicName": "Combinatory categorial grammar",
        "strength": 0.11142964393084803
      }, {
        "topicName": "Noun phrases",
        "strength": 0.07037692594176884
      }, {
        "topicName": "Rule-based parsing",
        "strength": 0.03867087710760122
      }, {
        "topicName": "Natural language & lexical semantics",
        "strength": 0.03758725090347298
      }, {
        "topicName": "Identifying unknown words",
        "strength": 0.02799594790725481
      }, {
        "topicName": "Experimental results",
        "strength": 0.023118635891592815
      }, {
        "topicName": "Language model",
        "strength": 0.01968678859578733
      }, {
        "topicName": "Natural languages & computational linguistics",
        "strength": 0.019069429387717357
      }, {
        "topicName": "Hidden Markov models",
        "strength": 0.016475161263826473
      }]
    }, {
      "author": "Clark, Alexander",
      "title": "Grammatical Inference And First Language Acquisition",
      "id": "W04-1304",
      "abstractText": "One argument for parametric models of language has been learnability in the context of first language acquisition. The claim is made that “logical” arguments from learnability theory require non-trivial constraints on the class of languages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. ",
      "year": "2004",
      "pedagogicalRole": null,
      "pageRankScore": 2.6263626E-5,
      "relevanceScore": 0.3835134493230021,
      "authorScore": 3.0,
      "relevantTopics": [{
        "topicName": "Language acquisition",
        "strength": 0.3835134493230021
      }, {
        "topicName": "Finite-state automata",
        "strength": 0.10918833974337672
      }, {
        "topicName": "Computational linguistics",
        "strength": 0.08894720314063038
      }, {
        "topicName": "Natural language & lexical semantics",
        "strength": 0.06933245114596524
      }, {
        "topicName": "Natural languages & computational linguistics",
        "strength": 0.05483562526536579
      }, {
        "topicName": "Context-free grammars",
        "strength": 0.04599520919617747
      }, {
        "topicName": "Gibbs sampling",
        "strength": 0.022399774181110377
      }, {
        "topicName": "User interfaces",
        "strength": 0.019224746707174507
      }, {
        "topicName": "Objective functions",
        "strength": 0.018424420646063346
      }, {
        "topicName": "Parallel corpora for machine translation",
        "strength": 0.017633728272515553
      }]
    }],
    "dependentTopics": [{
      "topic": [{
        "word": "phonological_rules",
        "value": 0.09284116327762604
      }, {
        "word": "syllable_structure",
        "value": 0.08612975478172302
      }, {
        "word": "word_boundaries",
        "value": 0.07186800986528397
      }, {
        "word": "phonetic_transcription",
        "value": 0.0542505607008934
      }, {
        "word": "syllable_boundaries",
        "value": 0.05257270857691765
      }, {
        "word": "word_accuracy",
        "value": 0.052013423293828964
      }, {
        "word": "pronunciation_dictionary",
        "value": 0.045861296355724335
      }, {
        "word": "vowels_consonants",
        "value": 0.045861296355724335
      }, {
        "word": "phoneme_sequence",
        "value": 0.04194631054997444
      }, {
        "word": "speech_synthesis",
        "value": 0.04082773998379707
      }, {
        "word": "primary_stress",
        "value": 0.04026845470070839
      }, {
        "word": "stress_assignment",
        "value": 0.039988815784454346
      }, {
        "word": "consonants_vowels",
        "value": 0.03942953050136566
      }, {
        "word": "writing_system",
        "value": 0.03942953050136566
      }, {
        "word": "phonological_features",
        "value": 0.038590602576732635
      }, {
        "word": "stress_pattern",
        "value": 0.03831096366047859
      }, {
        "word": "grapheme-to-phoneme_conversion",
        "value": 0.03691275045275688
      }, {
        "word": "consonant_vowel",
        "value": 0.03691275045275688
      }, {
        "word": "word_boundary",
        "value": 0.03579418361186981
      }, {
        "word": "syllable_boundary",
        "value": 0.03551454097032547
      }, {
        "word": "decision_tree",
        "value": 0.03467561677098274
      }],
      "documents": [{
        "author": "Divay, Michel; Vitale, Anthony J.",
        "title": "Algorithms For Grapheme-Phoneme Translation For English And French: Applications For Database Searches And Speech Synthesis",
        "id": "J97-4001",
        "abstractText": "Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis. These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, ff the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections. Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) ",
        "year": "1997",
        "pedagogicalRole": null,
        "pageRankScore": 1.8283338E-4,
        "relevanceScore": 0.36045981635495583,
        "authorScore": 0.0,
        "relevantTopics": [{
          "topicName": "Phonology & syllabification",
          "strength": 0.36045981635495583
        }, {
          "topicName": "Rule-based techniques",
          "strength": 0.11322963439156189
        }, {
          "topicName": "Identifying unknown words",
          "strength": 0.07974903754404604
        }, {
          "topicName": "Morphological analysis",
          "strength": 0.04719883530183425
        }, {
          "topicName": "Knowledge-based natural language processing",
          "strength": 0.028801758856851083
        }, {
          "topicName": "Probabilistic grammar & historical texts",
          "strength": 0.028551788886069886
        }, {
          "topicName": "Compound functional expressions",
          "strength": 0.02478488530697343
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.023281301255346255
        }, {
          "topicName": "Finite-state automata",
          "strength": 0.022808434186104183
        }, {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.022676831185659583
        }]
      }, {
        "author": "Carson-Berndsen, Julie",
        "title": "Unification And Transduction In Computational Phonology",
        "id": "C88-1023",
        "abstractText": "In this paper unification and. transduction mechanisms are applied in a new approach to phonological parsing. It is shown that unification in the sense of Kay as used in unification grammars, and Liansdnetion, a process deriving from automata theory, aft, bath valuable tools for use in computational pho.:elogy. By way of illustration, a brief outline of the allophonic parser described by Church is given. Then a linear unification parser for English syllables is introduced. This parser takes phonetic input in the form of feature bundles and uses phonological rules represented by networks of transduction relations together with unification, and an iterative finite-state process to produce phonemic output with marked syllable boundaries. A fundamental distinction is made between two domains: the. representations at the phonetic and phonological levels, and the processing of these representations. On. this basis, a distinction is made between networks of transduction relations (e.g. between allophones and phonemes), and a set of possible processors (i.e. parsers and transducers) for the interpretation of such networks. ",
        "year": "1988",
        "pedagogicalRole": null,
        "pageRankScore": 1.090198E-4,
        "relevanceScore": 0.38416319632216134,
        "authorScore": 0.0,
        "relevantTopics": [{
          "topicName": "Phonology & syllabification",
          "strength": 0.38416319632216134
        }, {
          "topicName": "Miscellany 23",
          "strength": 0.09114482968847844
        }, {
          "topicName": "Finite-state automata",
          "strength": 0.09041610278818478
        }, {
          "topicName": "Feature structures",
          "strength": 0.08664521858373556
        }, {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.06882549626377824
        }, {
          "topicName": "Neural networks",
          "strength": 0.04143839273041736
        }, {
          "topicName": "Rule-based parsing",
          "strength": 0.03471266136157835
        }, {
          "topicName": "Augmented transition network",
          "strength": 0.030135244372124875
        }, {
          "topicName": "Two-level morphological rules",
          "strength": 0.02109420598606587
        }, {
          "topicName": "Knowledge-based natural language processing",
          "strength": 0.016017316136842177
        }]
      }, {
        "author": "Jiampojamarn, Sittichai; Kondrak, Grzegorz; Sherif, Tarek",
        "title": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion",
        "id": "N07-1047",
        "abstractText": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek Sherif Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8, Canada {sj,kondrak,tarek}@cs.ualberta.ca Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word. ",
        "year": "2007",
        "pedagogicalRole": null,
        "pageRankScore": 8.2581886E-5,
        "relevanceScore": 0.5092235178431094,
        "authorScore": 3.67,
        "relevantTopics": [{
          "topicName": "Phonology & syllabification",
          "strength": 0.5092235178431094
        }, {
          "topicName": "Word alignment",
          "strength": 0.08332388838776748
        }, {
          "topicName": "Training and testing data sets",
          "strength": 0.08035552239999502
        }, {
          "topicName": "Experimental results",
          "strength": 0.06743625208741234
        }, {
          "topicName": "Hidden Markov models",
          "strength": 0.05988963652857628
        }, {
          "topicName": "Generative models & model selection",
          "strength": 0.0406890723066022
        }, {
          "topicName": "Language model",
          "strength": 0.027125496821011376
        }, {
          "topicName": "Feature selection and classifiers in machine learning",
          "strength": 0.020902852949071345
        }, {
          "topicName": "Machine transliteration",
          "strength": 0.01874741035484977
        }, {
          "topicName": "Objective functions",
          "strength": 0.015657666602526134
        }]
      }],
      "dependentTopics": [],
      "topicName": "Phonology & syllabification"
    }, {
      "topic": [{
        "word": "reading_level",
        "value": 0.10367892682552338
      }, {
        "word": "grade_level",
        "value": 0.09949832409620285
      }, {
        "word": "readability_assessment",
        "value": 0.08988294005393982
      }, {
        "word": "reading_comprehension",
        "value": 0.0852842777967453
      }, {
        "word": "reading_difficulty",
        "value": 0.05978260934352875
      }, {
        "word": "sentence_length",
        "value": 0.054765887558460236
      }, {
        "word": "grade_levels",
        "value": 0.05267558619379997
      }, {
        "word": "average_number",
        "value": 0.0510033443570137
      }, {
        "word": "number_words",
        "value": 0.04473244026303291
      }, {
        "word": "reading_levels",
        "value": 0.041806019842624664
      }, {
        "word": "syntactic_complexity",
        "value": 0.040133778005838394
      }, {
        "word": "collins-thompson_callan",
        "value": 0.035535115748643875
      }, {
        "word": "text_simplification",
        "value": 0.03344481438398361
      }, {
        "word": "language_learners",
        "value": 0.03302675485610962
      }, {
        "word": "text_readability",
        "value": 0.02842809446156025
      }, {
        "word": "english_language",
        "value": 0.024665551260113716
      }, {
        "word": "lexical_tightness",
        "value": 0.024665551260113716
      }, {
        "word": "grammatical_features",
        "value": 0.024247491732239723
      }, {
        "word": "language_models",
        "value": 0.024247491732239723
      }, {
        "word": "word_maturity",
        "value": 0.024247491732239723
      }, {
        "word": "number_syllables",
        "value": 0.024247491732239723
      }],
      "documents": [{
        "author": "Collins-Thompson, Kevyn; Callan, James, P.",
        "title": "A Language Modeling Approach To Predicting Reading Difficulty",
        "id": "N04-1025",
        "abstractText": "We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words). ",
        "year": "2004",
        "pedagogicalRole": null,
        "pageRankScore": 1.7574191E-4,
        "relevanceScore": 0.4046125042554952,
        "authorScore": 1.5,
        "relevantTopics": [{
          "topicName": "Readability / reading level",
          "strength": 0.4046125042554952
        }, {
          "topicName": "Training and testing data sets",
          "strength": 0.11243721447320687
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.10911058056295685
        }, {
          "topicName": "Language model",
          "strength": 0.07112091969008322
        }, {
          "topicName": "Generative models & model selection",
          "strength": 0.056000751531212985
        }, {
          "topicName": "Web pages",
          "strength": 0.054688452063473704
        }, {
          "topicName": "Feature selection and classifiers in machine learning",
          "strength": 0.04590593253157804
        }, {
          "topicName": "Experimental results",
          "strength": 0.03596827015817291
        }, {
          "topicName": "Identifying unknown words",
          "strength": 0.025329074372143694
        }, {
          "topicName": "\"Document classification, clustering, and categorization\"",
          "strength": 0.018952902453527324
        }]
      }, {
        "author": "Heilman, Michael; Collins-Thompson, Kevyn; Callan, James, P.; Eskenazi, Maxine",
        "title": "Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts",
        "id": "N07-1058",
        "abstractText": "Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts Michael J. Heilman Kevyn Collins- Jamie Callan Maxine Eskenazi Thompson Language Technologies Institute School of Computer Science Carnegie Mellon University 4502 Newell Simon Hall Pittsburgh, PA 15213-8213 {mheilman,kct,callan,max}@cs.cmu.edu This work evaluates a system that uses interpolated predictions of reading difficulty that are based on both vocabulary and grammatical features. The combined approach is compared to individual grammar and language modeling-based approaches. While the vocabulary-based language modeling approach outperformed the grammar-based approach, grammar-based predictions can be combined using confidence scores with the vocabulary-based predictions to produce more accurate predictions of reading difficulty for both first and second language texts. The results also indicate that grammatical features may play a more important role in second language readability than in first language readability. ",
        "year": "2007",
        "pedagogicalRole": null,
        "pageRankScore": 7.1899354E-5,
        "relevanceScore": 0.4026567289785623,
        "authorScore": 1.5,
        "relevantTopics": [{
          "topicName": "Readability / reading level",
          "strength": 0.4026567289785623
        }, {
          "topicName": "Feature selection and classifiers in machine learning",
          "strength": 0.07654374563774764
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.07298508409418387
        }, {
          "topicName": "Language model",
          "strength": 0.0668465453504487
        }, {
          "topicName": "Experimental results",
          "strength": 0.06243346567903139
        }, {
          "topicName": "Training and testing data sets",
          "strength": 0.06096511280517984
        }, {
          "topicName": "Text analysis",
          "strength": 0.034609884561410606
        }, {
          "topicName": "Noun phrases",
          "strength": 0.030944213221618097
        }, {
          "topicName": "Generative models & model selection",
          "strength": 0.02955287262249981
        }, {
          "topicName": "Morphological analysis",
          "strength": 0.021677011634759442
        }]
      }, {
        "author": "Heilman, Michael; Collins-Thompson, Kevyn; Eskenazi, Maxine",
        "title": "An Analysis of Statistical Models and Features for Reading Difficulty Prediction",
        "id": "W08-0909",
        "abstractText": "An Analysis of Statistical Models and Features for Reading Difficulty Prediction Michael Heilman, Kevyn Collins-Thompson and Maxine Eskenazi Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {mheilman,kct,max}@cs.cmu.edu A reading difficulty measure can be described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level. We describe a measure of readability that uses a combination of lexical features and grammatical features that are derived from subtrees of syntactic parses. We also tested statistical models for nominal, ordinal, and interval scales of measurement. The results indicate that a model for ordinal regression, such as the proportional odds model, using a combination of grammatical and lexical features is most effective at predicting reading difficulty. ",
        "year": "2008",
        "pedagogicalRole": null,
        "pageRankScore": 4.2211104E-5,
        "relevanceScore": 0.4116914227938045,
        "authorScore": 2.0,
        "relevantTopics": [{
          "topicName": "Readability / reading level",
          "strength": 0.4116914227938045
        }, {
          "topicName": "Feature selection and classifiers in machine learning",
          "strength": 0.09167977452818084
        }, {
          "topicName": "Training and testing data sets",
          "strength": 0.08570019392282775
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.06474872779020703
        }, {
          "topicName": "Generative models & model selection",
          "strength": 0.061005558601386456
        }, {
          "topicName": "Language model",
          "strength": 0.04645510649069326
        }, {
          "topicName": "Text analysis",
          "strength": 0.04364069609040192
        }, {
          "topicName": "Experimental results",
          "strength": 0.03411291759489363
        }, {
          "topicName": "Identifying unknown words",
          "strength": 0.02551760036641562
        }, {
          "topicName": "Noun phrases",
          "strength": 0.025495449380824205
        }]
      }],
      "dependentTopics": [],
      "topicName": "Readability / reading level"
    }, {
      "topic": [{
        "word": "description_length",
        "value": 0.3104838728904724
      }, {
        "word": "minimum_description",
        "value": 0.10201612859964371
      }, {
        "word": "mdl_principle",
        "value": 0.056854840368032455
      }, {
        "word": "key_terms",
        "value": 0.05645161122083664
      }, {
        "word": "length_mdl",
        "value": 0.045967742800712585
      }, {
        "word": "tree_cut",
        "value": 0.04516129195690155
      }, {
        "word": "li_abe",
        "value": 0.03387096896767616
      }, {
        "word": "number_bits",
        "value": 0.03306451439857483
      }, {
        "word": "case_frame",
        "value": 0.031854838132858276
      }, {
        "word": "unsupervised_learning",
        "value": 0.030241934582591057
      }, {
        "word": "stems_suffixes",
        "value": 0.027016129344701767
      }, {
        "word": "information_theory",
        "value": 0.025806451216340065
      }, {
        "word": "case_slots",
        "value": 0.025806451216340065
      }, {
        "word": "data_description",
        "value": 0.02500000037252903
      }, {
        "word": "global_key",
        "value": 0.024596774950623512
      }, {
        "word": "model_description",
        "value": 0.024596774950623512
      }, {
        "word": "cut_model",
        "value": 0.02177419327199459
      }, {
        "word": "length_principle",
        "value": 0.020967742428183556
      }, {
        "word": "local_key",
        "value": 0.019758064299821854
      }, {
        "word": "thesaurus_tree",
        "value": 0.019758064299821854
      }, {
        "word": "de_marcken",
        "value": 0.01895161345601082
      }],
      "documents": [{
        "author": "Goldsmith, John",
        "title": "Unsupervised Learning Of The Morphology Of A Natural Language",
        "id": "J01-2001",
        "abstractText": "This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words. We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not. The resulting grammar matches well the analysis that would be developed by a human morphologist. In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar. ",
        "year": "2001",
        "pedagogicalRole": null,
        "pageRankScore": 3.122403E-4,
        "relevanceScore": 0.38582696493651625,
        "authorScore": 0.0,
        "relevantTopics": [{
          "topicName": "Minimum description length",
          "strength": 0.38582696493651625
        }, {
          "topicName": "Morphological analysis",
          "strength": 0.13752827746519938
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.08015435397666494
        }, {
          "topicName": "Identifying unknown words",
          "strength": 0.06485105109174248
        }, {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.05190340784959399
        }, {
          "topicName": "Experimental results",
          "strength": 0.04115460187294831
        }, {
          "topicName": "Language model",
          "strength": 0.02154252376076377
        }, {
          "topicName": "Morphological segmentation",
          "strength": 0.018985184275964344
        }, {
          "topicName": "Rule-based parsing",
          "strength": 0.01748947518425815
        }, {
          "topicName": "Language acquisition",
          "strength": 0.01597652650890855
        }]
      }, {
        "author": "Li, Hang; Abe, Naoki",
        "title": "Generalizing Case Frames Using A Thesaurus And The MDL Principle",
        "id": "J98-2002",
        "abstractText": "A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods. ",
        "year": "1998",
        "pedagogicalRole": null,
        "pageRankScore": 1.8889048E-4,
        "relevanceScore": 0.5187811828403327,
        "authorScore": 7.0,
        "relevantTopics": [{
          "topicName": "Minimum description length",
          "strength": 0.5187811828403327
        }, {
          "topicName": "Experimental results",
          "strength": 0.09462081446558407
        }, {
          "topicName": "Language model",
          "strength": 0.06946430190677
        }, {
          "topicName": "Prepositional phrase attachment",
          "strength": 0.04725342470638673
        }, {
          "topicName": "Tree-adjoining grammar",
          "strength": 0.04702618726905159
        }, {
          "topicName": "Training and testing data sets",
          "strength": 0.0435095035669473
        }, {
          "topicName": "Generative models & model selection",
          "strength": 0.03654546951906376
        }, {
          "topicName": "Verb classes",
          "strength": 0.02482877472622946
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.01990683458785872
        }, {
          "topicName": "Semantic relations & knowledge",
          "strength": 0.0143332724228002
        }]
      }, {
        "author": "Snover, Matthew; Brent, Michael R.",
        "title": "A Bayesian Model For Morpheme And Paradigm Identification",
        "id": "P01-1063",
        "abstractText": "This paper describes a system for unsupervised learning of morphological affixes from texts or word lists. The system is composed of a generative probability model and a search algorithm. Experiments on the Wall Street Journal and the Hansard Corpus (French and English) demonstrate the effectiveness of this approach. The results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible. In addition, several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion. This formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space. ",
        "year": "2001",
        "pedagogicalRole": null,
        "pageRankScore": 5.4952958E-5,
        "relevanceScore": 0.4047843184844305,
        "authorScore": 3.5,
        "relevantTopics": [{
          "topicName": "Minimum description length",
          "strength": 0.4047843184844305
        }, {
          "topicName": "Morphological analysis",
          "strength": 0.17511610005819384
        }, {
          "topicName": "Identifying unknown words",
          "strength": 0.052349772688413884
        }, {
          "topicName": "Language model",
          "strength": 0.049466799823643944
        }, {
          "topicName": "Beam search & other search algorithms",
          "strength": 0.04426256207876938
        }, {
          "topicName": "Knowledge-based natural language processing",
          "strength": 0.04389744242673866
        }, {
          "topicName": "Experimental results",
          "strength": 0.04166343887314108
        }, {
          "topicName": "Tree-adjoining grammar",
          "strength": 0.031096671201320772
        }, {
          "topicName": "WMT system combination task",
          "strength": 0.027777723511432064
        }, {
          "topicName": "Computational linguistics",
          "strength": 0.024673617095128053
        }]
      }],
      "dependentTopics": [],
      "topicName": "Minimum description length"
    }],
    "topicName": "Language acquisition"
  }]
}