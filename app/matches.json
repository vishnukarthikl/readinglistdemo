{
  "keyword": "language_learning",
  "matchTopics": [
    {
      "topic": [
        {
          "word": "language_acquisition",
          "value": 0.20751342177391052
        },
        {
          "word": "language_learning",
          "value": 0.1520572453737259
        },
        {
          "word": "child_language",
          "value": 0.06746741384267807
        },
        {
          "word": "learning_algorithm",
          "value": 0.05954510718584061
        },
        {
          "word": "learning_process",
          "value": 0.050345003604888916
        },
        {
          "word": "word_learning",
          "value": 0.045489393174648285
        },
        {
          "word": "negative_evidence",
          "value": 0.03526705875992775
        },
        {
          "word": "child-directed_speech",
          "value": 0.035011500120162964
        },
        {
          "word": "language_learner",
          "value": 0.035011500120162964
        },
        {
          "word": "language_learners",
          "value": 0.03475594148039818
        },
        {
          "word": "learning_theory",
          "value": 0.031178124248981476
        },
        {
          "word": "computational_models",
          "value": 0.02913365699350834
        },
        {
          "word": "learning_system",
          "value": 0.027344748377799988
        },
        {
          "word": "learning_language",
          "value": 0.027089189738035202
        },
        {
          "word": "learning_model",
          "value": 0.026833631098270416
        },
        {
          "word": "children_learn",
          "value": 0.023766931146383286
        },
        {
          "word": "computational_model",
          "value": 0.023766931146383286
        },
        {
          "word": "learning_task",
          "value": 0.022744696587324142
        },
        {
          "word": "childes_database",
          "value": 0.022489137947559357
        },
        {
          "word": "language_development",
          "value": 0.02172246389091015
        },
        {
          "word": "young_children",
          "value": 0.021466905251145363
        }
      ],
      "documents": [
        {
          "author": "Yang, Charles D., ",
          "title": "A Selectionist Theory Of Language Acquisition",
          "id": "P99-1055",
          "abstractText": "This paper argues that developmental patterns in child language be taken seriously in computational models of language acquisition, and proposes a formal theory that meets this criterion. We first present developmental facts that are problematic for statistical learning approaches which assume no prior knowledge of grammar, and for traditional learnabil-ity models which assume the learner moves from one UG-defined grammar to another. In contrast, we view language acquisition as a population of grammars associated with \"weights\", that compete in a Darwinian selectionist process. Selection is made possible by the variational properties of individual grammars; specifically, their differential compatibility with the primary linguistic data in the environment. In addition to a convergence proof, we present empirical evidence in child language development, that a learner is best modeled as multiple grammars in coexistence and competition. ",
          "year": "1999",
          "pedagogicalRole": null,
          "pageRankScore": 5.2213963E-5,
          "relevanceScore": 0.5861056845876617,
          "authorScore": 0.0,
          "relevantTopics": [
            {
              "topicName": "Language acquisition",
              "strength": 0.5861056845876617
            },
            {
              "topicName": "Rule-based parsing",
              "strength": 0.06359550263544957
            },
            {
              "topicName": "Natural language & lexical semantics",
              "strength": 0.04608735816925561
            },
            {
              "topicName": "Computational linguistics",
              "strength": 0.044094375696947075
            },
            {
              "topicName": "Noun phrases",
              "strength": 0.039931142214077744
            },
            {
              "topicName": "Genetic algorithms & anaphora resolution",
              "strength": 0.024717169525345285
            },
            {
              "topicName": "Natural languages & computational linguistics",
              "strength": 0.022254646025394828
            },
            {
              "topicName": "Confusion sets",
              "strength": 0.019674379611831997
            },
            {
              "topicName": "Parallel corpora for machine translation",
              "strength": 0.017701452999750643
            },
            {
              "topicName": "Generative models & model selection",
              "strength": 0.01722359623561319
            }
          ],
          "index": 0
        },
        {
          "author": "Buttery, Paula, ",
          "title": "A Quantitative Evaluation Of Naturalistic Models Of Language Acquisition; The Efficiency Of The Triggering Learning Algorithm Compared To A Categorial Grammar Learner",
          "id": "W04-1301",
          "abstractText": "Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efficiency against more reliable convergence to the target grammar. ",
          "year": "2004",
          "pedagogicalRole": null,
          "pageRankScore": 3.229716E-5,
          "relevanceScore": 0.5381193166905334,
          "authorScore": 0.0,
          "relevantTopics": [
            {
              "topicName": "Language acquisition",
              "strength": 0.5381193166905334
            },
            {
              "topicName": "Combinatory categorial grammar",
              "strength": 0.11142964393084803
            },
            {
              "topicName": "Noun phrases",
              "strength": 0.07037692594176884
            },
            {
              "topicName": "Rule-based parsing",
              "strength": 0.03867087710760122
            },
            {
              "topicName": "Natural language & lexical semantics",
              "strength": 0.03758725090347298
            },
            {
              "topicName": "Identifying unknown words",
              "strength": 0.02799594790725481
            },
            {
              "topicName": "Experimental results",
              "strength": 0.023118635891592815
            },
            {
              "topicName": "Language model",
              "strength": 0.01968678859578733
            },
            {
              "topicName": "Natural languages & computational linguistics",
              "strength": 0.019069429387717357
            },
            {
              "topicName": "Hidden Markov models",
              "strength": 0.016475161263826473
            }
          ],
          "index": 1
        },
        {
          "author": "Clark, Alexander, ",
          "title": "Grammatical Inference And First Language Acquisition",
          "id": "W04-1304",
          "abstractText": "One argument for parametric models of language has been learnability in the context of first language acquisition. The claim is made that “logical” arguments from learnability theory require non-trivial constraints on the class of languages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. ",
          "year": "2004",
          "pedagogicalRole": null,
          "pageRankScore": 2.6263626E-5,
          "relevanceScore": 0.3835134493230021,
          "authorScore": 3.0,
          "relevantTopics": [
            {
              "topicName": "Language acquisition",
              "strength": 0.3835134493230021
            },
            {
              "topicName": "Finite-state automata",
              "strength": 0.10918833974337672
            },
            {
              "topicName": "Computational linguistics",
              "strength": 0.08894720314063038
            },
            {
              "topicName": "Natural language & lexical semantics",
              "strength": 0.06933245114596524
            },
            {
              "topicName": "Natural languages & computational linguistics",
              "strength": 0.05483562526536579
            },
            {
              "topicName": "Context-free grammars",
              "strength": 0.04599520919617747
            },
            {
              "topicName": "Gibbs sampling",
              "strength": 0.022399774181110377
            },
            {
              "topicName": "User interfaces",
              "strength": 0.019224746707174507
            },
            {
              "topicName": "Objective functions",
              "strength": 0.018424420646063346
            },
            {
              "topicName": "Parallel corpora for machine translation",
              "strength": 0.017633728272515553
            }
          ],
          "index": 2
        }
      ],
      "dependentTopics": [
        {
          "topic": [
            {
              "word": "reading_level",
              "value": 0.10367892682552338
            },
            {
              "word": "grade_level",
              "value": 0.09949832409620285
            },
            {
              "word": "readability_assessment",
              "value": 0.08988294005393982
            },
            {
              "word": "reading_comprehension",
              "value": 0.0852842777967453
            },
            {
              "word": "reading_difficulty",
              "value": 0.05978260934352875
            },
            {
              "word": "sentence_length",
              "value": 0.054765887558460236
            },
            {
              "word": "grade_levels",
              "value": 0.05267558619379997
            },
            {
              "word": "average_number",
              "value": 0.0510033443570137
            },
            {
              "word": "number_words",
              "value": 0.04473244026303291
            },
            {
              "word": "reading_levels",
              "value": 0.041806019842624664
            },
            {
              "word": "syntactic_complexity",
              "value": 0.040133778005838394
            },
            {
              "word": "collins-thompson_callan",
              "value": 0.035535115748643875
            },
            {
              "word": "text_simplification",
              "value": 0.03344481438398361
            },
            {
              "word": "language_learners",
              "value": 0.03302675485610962
            },
            {
              "word": "text_readability",
              "value": 0.02842809446156025
            },
            {
              "word": "english_language",
              "value": 0.024665551260113716
            },
            {
              "word": "lexical_tightness",
              "value": 0.024665551260113716
            },
            {
              "word": "grammatical_features",
              "value": 0.024247491732239723
            },
            {
              "word": "language_models",
              "value": 0.024247491732239723
            },
            {
              "word": "word_maturity",
              "value": 0.024247491732239723
            },
            {
              "word": "number_syllables",
              "value": 0.024247491732239723
            }
          ],
          "documents": [
            {
              "author": "Collins-Thompson, Kevyn; Callan, James, P., ",
              "title": "A Language Modeling Approach To Predicting Reading Difficulty",
              "id": "N04-1025",
              "abstractText": "We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words). ",
              "year": "2004",
              "pedagogicalRole": null,
              "pageRankScore": 1.7574191E-4,
              "relevanceScore": 0.4046125042554952,
              "authorScore": 1.5,
              "relevantTopics": [
                {
                  "topicName": "Readability / reading level",
                  "strength": 0.4046125042554952
                },
                {
                  "topicName": "Training and testing data sets",
                  "strength": 0.11243721447320687
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.10911058056295685
                },
                {
                  "topicName": "Language model",
                  "strength": 0.07112091969008322
                },
                {
                  "topicName": "Generative models & model selection",
                  "strength": 0.056000751531212985
                },
                {
                  "topicName": "Web pages",
                  "strength": 0.054688452063473704
                },
                {
                  "topicName": "Feature selection and classifiers in machine learning",
                  "strength": 0.04590593253157804
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.03596827015817291
                },
                {
                  "topicName": "Identifying unknown words",
                  "strength": 0.025329074372143694
                },
                {
                  "topicName": "\"Document classification, clustering, and categorization\"",
                  "strength": 0.018952902453527324
                }
              ],
              "index": 3
            },
            {
              "author": "Heilman, Michael; Collins-Thompson, Kevyn; Callan, James, P.; Eskenazi, Maxine, ",
              "title": "Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts",
              "id": "N07-1058",
              "abstractText": "Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts Michael J. Heilman Kevyn Collins- Jamie Callan Maxine Eskenazi Thompson Language Technologies Institute School of Computer Science Carnegie Mellon University 4502 Newell Simon Hall Pittsburgh, PA 15213-8213 {mheilman,kct,callan,max}@cs.cmu.edu This work evaluates a system that uses interpolated predictions of reading difficulty that are based on both vocabulary and grammatical features. The combined approach is compared to individual grammar and language modeling-based approaches. While the vocabulary-based language modeling approach outperformed the grammar-based approach, grammar-based predictions can be combined using confidence scores with the vocabulary-based predictions to produce more accurate predictions of reading difficulty for both first and second language texts. The results also indicate that grammatical features may play a more important role in second language readability than in first language readability. ",
              "year": "2007",
              "pedagogicalRole": null,
              "pageRankScore": 7.1899354E-5,
              "relevanceScore": 0.4026567289785623,
              "authorScore": 1.5,
              "relevantTopics": [
                {
                  "topicName": "Readability / reading level",
                  "strength": 0.4026567289785623
                },
                {
                  "topicName": "Feature selection and classifiers in machine learning",
                  "strength": 0.07654374563774764
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.07298508409418387
                },
                {
                  "topicName": "Language model",
                  "strength": 0.0668465453504487
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.06243346567903139
                },
                {
                  "topicName": "Training and testing data sets",
                  "strength": 0.06096511280517984
                },
                {
                  "topicName": "Text analysis",
                  "strength": 0.034609884561410606
                },
                {
                  "topicName": "Noun phrases",
                  "strength": 0.030944213221618097
                },
                {
                  "topicName": "Generative models & model selection",
                  "strength": 0.02955287262249981
                },
                {
                  "topicName": "Morphological analysis",
                  "strength": 0.021677011634759442
                }
              ],
              "index": 4
            },
            {
              "author": "Heilman, Michael; Collins-Thompson, Kevyn; Eskenazi, Maxine, ",
              "title": "An Analysis of Statistical Models and Features for Reading Difficulty Prediction",
              "id": "W08-0909",
              "abstractText": "An Analysis of Statistical Models and Features for Reading Difficulty Prediction Michael Heilman, Kevyn Collins-Thompson and Maxine Eskenazi Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {mheilman,kct,max}@cs.cmu.edu A reading difficulty measure can be described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level. We describe a measure of readability that uses a combination of lexical features and grammatical features that are derived from subtrees of syntactic parses. We also tested statistical models for nominal, ordinal, and interval scales of measurement. The results indicate that a model for ordinal regression, such as the proportional odds model, using a combination of grammatical and lexical features is most effective at predicting reading difficulty. ",
              "year": "2008",
              "pedagogicalRole": null,
              "pageRankScore": 4.2211104E-5,
              "relevanceScore": 0.4116914227938045,
              "authorScore": 2.0,
              "relevantTopics": [
                {
                  "topicName": "Readability / reading level",
                  "strength": 0.4116914227938045
                },
                {
                  "topicName": "Feature selection and classifiers in machine learning",
                  "strength": 0.09167977452818084
                },
                {
                  "topicName": "Training and testing data sets",
                  "strength": 0.08570019392282775
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.06474872779020703
                },
                {
                  "topicName": "Generative models & model selection",
                  "strength": 0.061005558601386456
                },
                {
                  "topicName": "Language model",
                  "strength": 0.04645510649069326
                },
                {
                  "topicName": "Text analysis",
                  "strength": 0.04364069609040192
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.03411291759489363
                },
                {
                  "topicName": "Identifying unknown words",
                  "strength": 0.02551760036641562
                },
                {
                  "topicName": "Noun phrases",
                  "strength": 0.025495449380824205
                }
              ],
              "index": 5
            }
          ],
          "dependentTopics": [],
          "topicName": "Readability / reading level"
        },
        {
          "topic": [
            {
              "word": "description_length",
              "value": 0.3104838728904724
            },
            {
              "word": "minimum_description",
              "value": 0.10201612859964371
            },
            {
              "word": "mdl_principle",
              "value": 0.056854840368032455
            },
            {
              "word": "key_terms",
              "value": 0.05645161122083664
            },
            {
              "word": "length_mdl",
              "value": 0.045967742800712585
            },
            {
              "word": "tree_cut",
              "value": 0.04516129195690155
            },
            {
              "word": "li_abe",
              "value": 0.03387096896767616
            },
            {
              "word": "number_bits",
              "value": 0.03306451439857483
            },
            {
              "word": "case_frame",
              "value": 0.031854838132858276
            },
            {
              "word": "unsupervised_learning",
              "value": 0.030241934582591057
            },
            {
              "word": "stems_suffixes",
              "value": 0.027016129344701767
            },
            {
              "word": "information_theory",
              "value": 0.025806451216340065
            },
            {
              "word": "case_slots",
              "value": 0.025806451216340065
            },
            {
              "word": "data_description",
              "value": 0.02500000037252903
            },
            {
              "word": "global_key",
              "value": 0.024596774950623512
            },
            {
              "word": "model_description",
              "value": 0.024596774950623512
            },
            {
              "word": "cut_model",
              "value": 0.02177419327199459
            },
            {
              "word": "length_principle",
              "value": 0.020967742428183556
            },
            {
              "word": "local_key",
              "value": 0.019758064299821854
            },
            {
              "word": "thesaurus_tree",
              "value": 0.019758064299821854
            },
            {
              "word": "de_marcken",
              "value": 0.01895161345601082
            }
          ],
          "documents": [
            {
              "author": "Goldsmith, John, ",
              "title": "Unsupervised Learning Of The Morphology Of A Natural Language",
              "id": "J01-2001",
              "abstractText": "This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words. We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not. The resulting grammar matches well the analysis that would be developed by a human morphologist. In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar. ",
              "year": "2001",
              "pedagogicalRole": null,
              "pageRankScore": 3.122403E-4,
              "relevanceScore": 0.38582696493651625,
              "authorScore": 0.0,
              "relevantTopics": [
                {
                  "topicName": "Minimum description length",
                  "strength": 0.38582696493651625
                },
                {
                  "topicName": "Morphological analysis",
                  "strength": 0.13752827746519938
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.08015435397666494
                },
                {
                  "topicName": "Identifying unknown words",
                  "strength": 0.06485105109174248
                },
                {
                  "topicName": "Natural language & lexical semantics",
                  "strength": 0.05190340784959399
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.04115460187294831
                },
                {
                  "topicName": "Language model",
                  "strength": 0.02154252376076377
                },
                {
                  "topicName": "Morphological segmentation",
                  "strength": 0.018985184275964344
                },
                {
                  "topicName": "Rule-based parsing",
                  "strength": 0.01748947518425815
                },
                {
                  "topicName": "Language acquisition",
                  "strength": 0.01597652650890855
                }
              ],
              "index": 6
            },
            {
              "author": "Li, Hang; Abe, Naoki, ",
              "title": "Generalizing Case Frames Using A Thesaurus And The MDL Principle",
              "id": "J98-2002",
              "abstractText": "A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods. ",
              "year": "1998",
              "pedagogicalRole": null,
              "pageRankScore": 1.8889048E-4,
              "relevanceScore": 0.5187811828403327,
              "authorScore": 7.0,
              "relevantTopics": [
                {
                  "topicName": "Minimum description length",
                  "strength": 0.5187811828403327
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.09462081446558407
                },
                {
                  "topicName": "Language model",
                  "strength": 0.06946430190677
                },
                {
                  "topicName": "Prepositional phrase attachment",
                  "strength": 0.04725342470638673
                },
                {
                  "topicName": "Tree-adjoining grammar",
                  "strength": 0.04702618726905159
                },
                {
                  "topicName": "Training and testing data sets",
                  "strength": 0.0435095035669473
                },
                {
                  "topicName": "Generative models & model selection",
                  "strength": 0.03654546951906376
                },
                {
                  "topicName": "Verb classes",
                  "strength": 0.02482877472622946
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.01990683458785872
                },
                {
                  "topicName": "Semantic relations & knowledge",
                  "strength": 0.0143332724228002
                }
              ],
              "index": 7
            },
            {
              "author": "Snover, Matthew; Brent, Michael R., ",
              "title": "A Bayesian Model For Morpheme And Paradigm Identification",
              "id": "P01-1063",
              "abstractText": "This paper describes a system for unsupervised learning of morphological affixes from texts or word lists. The system is composed of a generative probability model and a search algorithm. Experiments on the Wall Street Journal and the Hansard Corpus (French and English) demonstrate the effectiveness of this approach. The results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible. In addition, several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion. This formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space. ",
              "year": "2001",
              "pedagogicalRole": null,
              "pageRankScore": 5.4952958E-5,
              "relevanceScore": 0.4047843184844305,
              "authorScore": 3.5,
              "relevantTopics": [
                {
                  "topicName": "Minimum description length",
                  "strength": 0.4047843184844305
                },
                {
                  "topicName": "Morphological analysis",
                  "strength": 0.17511610005819384
                },
                {
                  "topicName": "Identifying unknown words",
                  "strength": 0.052349772688413884
                },
                {
                  "topicName": "Language model",
                  "strength": 0.049466799823643944
                },
                {
                  "topicName": "Beam search & other search algorithms",
                  "strength": 0.04426256207876938
                },
                {
                  "topicName": "Knowledge-based natural language processing",
                  "strength": 0.04389744242673866
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.04166343887314108
                },
                {
                  "topicName": "Tree-adjoining grammar",
                  "strength": 0.031096671201320772
                },
                {
                  "topicName": "WMT system combination task",
                  "strength": 0.027777723511432064
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.024673617095128053
                }
              ],
              "index": 8
            }
          ],
          "dependentTopics": [],
          "topicName": "Minimum description length"
        },
        {
          "topic": [
            {
              "word": "virtual_world",
              "value": 0.11442112177610397
            },
            {
              "word": "virtual_environment",
              "value": 0.08666215091943741
            },
            {
              "word": "adnominal_constituents",
              "value": 0.06770480424165726
            },
            {
              "word": "route_instructions",
              "value": 0.059580229222774506
            },
            {
              "word": "semantic_map",
              "value": 0.059580229222774506
            },
            {
              "word": "turn_left",
              "value": 0.05890318378806114
            },
            {
              "word": "virtual_environments",
              "value": 0.05280974879860878
            },
            {
              "word": "give_challenge",
              "value": 0.046716317534446716
            },
            {
              "word": "navigation_system",
              "value": 0.04536221921443939
            },
            {
              "word": "nlg_systems",
              "value": 0.04468517377972603
            },
            {
              "word": "route_directions",
              "value": 0.03926878795027733
            },
            {
              "word": "lexical_items",
              "value": 0.038591738790273666
            },
            {
              "word": "nlg_system",
              "value": 0.03656059503555298
            },
            {
              "word": "head_nouns",
              "value": 0.03452945128083229
            },
            {
              "word": "surgical_deed",
              "value": 0.033175356686115265
            },
            {
              "word": "game_world",
              "value": 0.031144211068749428
            },
            {
              "word": "virtual_instructor",
              "value": 0.030467163771390915
            },
            {
              "word": "belief_state",
              "value": 0.030467163771390915
            },
            {
              "word": "instructions_virtual",
              "value": 0.029790114611387253
            },
            {
              "word": "adjectival_nouns",
              "value": 0.029790114611387253
            },
            {
              "word": "communication_channel",
              "value": 0.029790114611387253
            }
          ],
          "documents": [
            {
              "author": "Frederking, Robert E.; Nirenburg, Sergei, ",
              "title": "Three Heads Are Better Than One",
              "id": "A94-1016",
              "abstractText": "Current MT systems, whatever translation method they employ, do not reach an optimal output on free text. In part, this is due to the inherent problems of a particular method — for instance, the inability of statistics-based MT to take into account long-distance dependencies, the difficulty in achieving extremely broad coverage in knowledge-based MT systems, or the reliance of most transfer-oriented MT systems on similarities in syntactic structures of the source and the target languages. Our hypothesis is that if an MT environment can use the best results from a variety of MT systems working simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart data structure and select the overall best translation using a set of simple heuristics. ",
              "year": "1994",
              "pedagogicalRole": null,
              "pageRankScore": 1.8371355E-4,
              "relevanceScore": 0.45287249602934626,
              "authorScore": 6.5,
              "relevantTopics": [
                {
                  "topicName": "Virtual world",
                  "strength": 0.45287249602934626
                },
                {
                  "topicName": "Parallel corpora for machine translation",
                  "strength": 0.10748207304285179
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.09339370690961898
                },
                {
                  "topicName": "Knowledge-based natural language processing",
                  "strength": 0.04750280176526811
                },
                {
                  "topicName": "Machine translation systems",
                  "strength": 0.045699069208667904
                },
                {
                  "topicName": "Rule-based parsing",
                  "strength": 0.04335083648061003
                },
                {
                  "topicName": "Beam search & other search algorithms",
                  "strength": 0.03610152581536706
                },
                {
                  "topicName": "Human assessment",
                  "strength": 0.032917035233456696
                },
                {
                  "topicName": "Identifying unknown words",
                  "strength": 0.029033661618538136
                },
                {
                  "topicName": "Text analysis",
                  "strength": 0.020138071792617226
                }
              ],
              "index": 9
            },
            {
              "author": "Koller, Alexander; Striegnitz, Kristina; Byron, Donna K.; Cassell, Justine; Dale, Robert; Dalzel-Job, Sara; Moore, Johanna D.; Oberlander, Jon, ",
              "title": "Validating the web-based evaluation of NLG systems",
              "id": "P09-2076",
              "abstractText": "Validating the web-based evaluation of NLG systems Alexander Koller Kristina Striegnitz Donna Byron Justine Cassell Saarland U. Union College Northeastern U. Northwestern U. ller@mmci.uni-saarland.de striegnk@union.edu dbyron@ccs.neu.edu justine@northwestern.e Robert Dale Sara Dalzel-Job Jon Oberlander Johanna Moore Macquarie U. U. of Edinburgh U. of Edinburgh U. of Edinburgh Robert.Dale@mq.edu.au S . Dalzel-Job@ sms.ed.ac.uk {J.Oberlander| J .Moore}@ed.ac.uk The GIVE Challenge is a recent shared task in which NLG systems are evaluated over the Internet. ",
              "year": "2009",
              "pedagogicalRole": null,
              "pageRankScore": 5.025448E-5,
              "relevanceScore": 0.485238578311597,
              "authorScore": 7.63,
              "relevantTopics": [
                {
                  "topicName": "Virtual world",
                  "strength": 0.485238578311597
                },
                {
                  "topicName": "Human assessment",
                  "strength": 0.19249072634741599
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.07666009674076474
                },
                {
                  "topicName": "Natural language generation systems",
                  "strength": 0.049772274661039465
                },
                {
                  "topicName": "Dialog systems & user satisfaction",
                  "strength": 0.047299441090217983
                },
                {
                  "topicName": "Training and testing data sets",
                  "strength": 0.03760031783415556
                },
                {
                  "topicName": "Knowledge-based natural language processing",
                  "strength": 0.027591700863296466
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.02531369504095586
                },
                {
                  "topicName": "Miscellany 17",
                  "strength": 0.014104015392149364
                },
                {
                  "topicName": "Gender classification",
                  "strength": 0.013274606912683331
                }
              ],
              "index": 10
            },
            {
              "author": "Byron, Donna K.; Koller, Alexander; Striegnitz, Kristina; Cassell, Justine; Dale, Robert; Moore, Johanna D.; Oberlander, Jon, ",
              "title": "Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE)",
              "id": "W09-0628",
              "abstractText": "Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE) Donna Byron Northeastern University dbyron@ccs.neu.edu Alexander Koller Saarland University koller@mmci.uni-saarland.de Kristina Striegnitz Union College striegnk@union.edu ",
              "year": "2009",
              "pedagogicalRole": null,
              "pageRankScore": 4.4124954E-5,
              "relevanceScore": 0.6099420107736772,
              "authorScore": 8.71,
              "relevantTopics": [
                {
                  "topicName": "Virtual world",
                  "strength": 0.6099420107736772
                },
                {
                  "topicName": "Human assessment",
                  "strength": 0.11720071958718409
                },
                {
                  "topicName": "Computational linguistics",
                  "strength": 0.05831119109605089
                },
                {
                  "topicName": "Knowledge-based natural language processing",
                  "strength": 0.05540228292720066
                },
                {
                  "topicName": "Dialog systems & user satisfaction",
                  "strength": 0.028358812424317256
                },
                {
                  "topicName": "Natural language generation systems",
                  "strength": 0.020790673874595437
                },
                {
                  "topicName": "Miscellany 17",
                  "strength": 0.019844932402361636
                },
                {
                  "topicName": "Experimental results",
                  "strength": 0.018670608736723632
                },
                {
                  "topicName": "User interfaces",
                  "strength": 0.017515546956130373
                },
                {
                  "topicName": "Spatial referring expressions",
                  "strength": 0.014182812767805187
                }
              ],
              "index": 11
            }
          ],
          "dependentTopics": [],
          "topicName": "Virtual world"
        }
      ],
      "topicName": "Language acquisition"
    }
  ],
  "graphResponse": {
    "nodes": [
      {
        "id": 293,
        "label": "Readability / reading level",
        "matched": false
      },
      {
        "id": 54,
        "label": "Minimum description length",
        "matched": false
      },
      {
        "id": 105,
        "label": "Virtual world",
        "matched": false
      },
      {
        "id": 267,
        "label": "Language acquisition",
        "matched": true
      }
    ],
    "edges": [
      {
        "from": 267,
        "to": 54,
        "value": 54.0
      },
      {
        "from": 267,
        "to": 105,
        "value": 105.0
      },
      {
        "from": 267,
        "to": 293,
        "value": 293.0
      }
    ]
  },
  "baseLineDocuments": [
    {
      "author": "Yang, Charles D., ",
      "title": "A Selectionist Theory Of Language Acquisition",
      "id": "P99-1055",
      "abstractText": "This paper argues that developmental patterns in child language be taken seriously in computational models of language acquisition, and proposes a formal theory that meets this criterion. We first present developmental facts that are problematic for statistical learning approaches which assume no prior knowledge of grammar, and for traditional learnabil-ity models which assume the learner moves from one UG-defined grammar to another. In contrast, we view language acquisition as a population of grammars associated with \"weights\", that compete in a Darwinian selectionist process. Selection is made possible by the variational properties of individual grammars; specifically, their differential compatibility with the primary linguistic data in the environment. In addition to a convergence proof, we present empirical evidence in child language development, that a learner is best modeled as multiple grammars in coexistence and competition. ",
      "year": "1999",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.5861056845876617
        },
        {
          "topicName": "Rule-based parsing",
          "strength": 0.06359550263544957
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.04608735816925561
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.044094375696947075
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.039931142214077744
        },
        {
          "topicName": "Genetic algorithms & anaphora resolution",
          "strength": 0.024717169525345285
        },
        {
          "topicName": "Natural languages & computational linguistics",
          "strength": 0.022254646025394828
        },
        {
          "topicName": "Confusion sets",
          "strength": 0.019674379611831997
        },
        {
          "topicName": "Parallel corpora for machine translation",
          "strength": 0.017701452999750643
        },
        {
          "topicName": "Generative models & model selection",
          "strength": 0.01722359623561319
        }
      ],
      "index": 0
    },
    {
      "author": "Buttery, Paula, ",
      "title": "A Quantitative Evaluation Of Naturalistic Models Of Language Acquisition; The Efficiency Of The Triggering Learning Algorithm Compared To A Categorial Grammar Learner",
      "id": "W04-1301",
      "abstractText": "Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efficiency against more reliable convergence to the target grammar. ",
      "year": "2004",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.5381193166905334
        },
        {
          "topicName": "Combinatory categorial grammar",
          "strength": 0.11142964393084803
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.07037692594176884
        },
        {
          "topicName": "Rule-based parsing",
          "strength": 0.03867087710760122
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.03758725090347298
        },
        {
          "topicName": "Identifying unknown words",
          "strength": 0.02799594790725481
        },
        {
          "topicName": "Experimental results",
          "strength": 0.023118635891592815
        },
        {
          "topicName": "Language model",
          "strength": 0.01968678859578733
        },
        {
          "topicName": "Natural languages & computational linguistics",
          "strength": 0.019069429387717357
        },
        {
          "topicName": "Hidden Markov models",
          "strength": 0.016475161263826473
        }
      ],
      "index": 1
    },
    {
      "author": "Sakas, William Gregory, ",
      "title": "A Word-Order Database For Testing Computational Models Of Language Acquisition",
      "id": "P03-1053",
      "abstractText": "An investment of effort over the last two years has begun to produce a wealth of data concerning computational psycholinguistic models of syntax acquisition. The data is generated by running simulations on a recently completed database of word order patterns from over 3,000 abstract languages. This article presents the design of the database which contains sentence patterns, grammars and derivations that can be used to test acquisition models from widely divergent paradigms. The domain is generated from grammars that are linguistically motivated by current syntactic theory and the sentence patterns have been validated as psychologically/developmentally plausible by checking their frequency of occurrence in corpora of child-directed speech. A small case-study simulation is also presented. ",
      "year": "2003",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.3920287414576951
        },
        {
          "topicName": "Rule-based parsing",
          "strength": 0.08748495413754849
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.05228015571264065
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.050681592973006694
        },
        {
          "topicName": "Constraint networks",
          "strength": 0.04470563837010977
        },
        {
          "topicName": "Information extraction patterns",
          "strength": 0.03440836341221451
        },
        {
          "topicName": "Knowledge-based natural language processing",
          "strength": 0.03353533942098462
        },
        {
          "topicName": "User interfaces",
          "strength": 0.030002637864395637
        },
        {
          "topicName": "Cultural heritage & extragrammaticality",
          "strength": 0.027512978895655592
        },
        {
          "topicName": "Parallel corpora for machine translation",
          "strength": 0.023626883049665145
        }
      ],
      "index": 2
    },
    {
      "author": "Clark, Alexander, ",
      "title": "Grammatical Inference And First Language Acquisition",
      "id": "W04-1304",
      "abstractText": "One argument for parametric models of language has been learnability in the context of first language acquisition. The claim is made that “logical” arguments from learnability theory require non-trivial constraints on the class of languages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. ",
      "year": "2004",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 3.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.3835134493230021
        },
        {
          "topicName": "Finite-state automata",
          "strength": 0.10918833974337672
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.08894720314063038
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.06933245114596524
        },
        {
          "topicName": "Natural languages & computational linguistics",
          "strength": 0.05483562526536579
        },
        {
          "topicName": "Context-free grammars",
          "strength": 0.04599520919617747
        },
        {
          "topicName": "Gibbs sampling",
          "strength": 0.022399774181110377
        },
        {
          "topicName": "User interfaces",
          "strength": 0.019224746707174507
        },
        {
          "topicName": "Objective functions",
          "strength": 0.018424420646063346
        },
        {
          "topicName": "Parallel corpora for machine translation",
          "strength": 0.017633728272515553
        }
      ],
      "index": 3
    },
    {
      "author": "Rappoport, Ari; Sheinman, Vera, ",
      "title": "A Second Language Acquisition Model Using Example Generalization And Concept Categories",
      "id": "W05-0506",
      "abstractText": "We present a computational model of acquiring a second language from example sentences. Our learning algorithms build a construction grammar language model, and generalize using form-based patterns and the learners conceptual ystem. We use a unique professional language learning corpus, and show that substantial reliable learning can be achieved even though the corpus is very small. The model is applied to assisting the authoring of Japanese language learning corpora. ",
      "year": "2005",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 2.5,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.3679071983971715
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.07011690991701232
        },
        {
          "topicName": "Generative models & model selection",
          "strength": 0.04763005300634228
        },
        {
          "topicName": "Experimental results",
          "strength": 0.04360633053487866
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.04309498844635826
        },
        {
          "topicName": "Knowledge-based natural language processing",
          "strength": 0.035561191324456454
        },
        {
          "topicName": "Identifying unknown words",
          "strength": 0.03554153544983013
        },
        {
          "topicName": "Computational linguistics (discipline)",
          "strength": 0.03218448121630477
        },
        {
          "topicName": "Miscellany 3",
          "strength": 0.03217554977590255
        },
        {
          "topicName": "Eye tracking",
          "strength": 0.03077619039819496
        }
      ],
      "index": 4
    },
    {
      "author": "Gambell, Timothy; Yang, Charles D., ",
      "title": "Statistics Learning And Universal Grammar: Modeling Word Segmentation",
      "id": "W04-1307",
      "abstractText": "Two facts about language learning are indisputable. First, only a human baby, but not her pet kitten, can learn a language. It is clear, then, that there must be some element in our biology that accounts for this unique ability. Chomsky’s Universal Grammar (UG), an innate form of knowledge specific to language, is an account of what this ability is. This position gains support from formal learning theory [1- 3], which sharpens the logical conclusion [4,5] that no (realistically efficient) learning is possible without priori restrictions on the learning space. Second, it is also clear that no matter how much of a head start the child has through UG, language is learned. Phonology, lexicon, and grammar, while governed by universal principles and constraints, do vary from language to language, and they must be learned on the basis of linguistic experience. In other words–indeed a truism –both endowment and learning contribute to language acquisition, the result of which is extremely sophisticated body of linguistic knowledge. Consequently, both must be taken in account, explicitly, in a theory of language acquisition [6,7]. Controversies arise when it comes to the relative contributions by innate knowledge and experience-based learning. ",
      "year": "2004",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.39737788562278453
        },
        {
          "topicName": "Phonology & syllabification",
          "strength": 0.12381772355346937
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.08723310126037255
        },
        {
          "topicName": "Chinese word segmentation",
          "strength": 0.045962218559809226
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.04135534882564233
        },
        {
          "topicName": "Identifying unknown words",
          "strength": 0.04115650984969524
        },
        {
          "topicName": "Minimum description length",
          "strength": 0.03195474169196294
        },
        {
          "topicName": "Training and testing data sets",
          "strength": 0.027287314078157297
        },
        {
          "topicName": "Miscellany 28",
          "strength": 0.025961296131875287
        },
        {
          "topicName": "Experimental results",
          "strength": 0.023480627176808874
        }
      ],
      "index": 5
    },
    {
      "author": "Sahakian, Sam; Snyder, Benjamin, ",
      "title": "Automatically Learning Measures of Child Language Development",
      "id": "P12-2019",
      "abstractText": "We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction  experiments. In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. We then learn a more general developmental  index by applying our method across children,  predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language  development, showing improvements in age prediction performance. ",
      "year": "2012",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 1.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.36259045036441334
        },
        {
          "topicName": "Training and testing data sets",
          "strength": 0.10363587004138688
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.06708758316116288
        },
        {
          "topicName": "Oral proficiency testing",
          "strength": 0.04791054721218402
        },
        {
          "topicName": "Readability / reading level",
          "strength": 0.04415532560044969
        },
        {
          "topicName": "Feature selection and classifiers in machine learning",
          "strength": 0.04334581324745204
        },
        {
          "topicName": "Miscellany 10",
          "strength": 0.04321256886899356
        },
        {
          "topicName": "Machine translation evaluation",
          "strength": 0.04228696739541633
        },
        {
          "topicName": "Experimental results",
          "strength": 0.0371201913490458
        },
        {
          "topicName": "Objective functions",
          "strength": 0.031065253391691857
        }
      ],
      "index": 6
    },
    {
      "author": "Briscoe, Ted, ",
      "title": "Co-Evolution Of Language And Of The Language Acquisition Device",
      "id": "P97-1054",
      "abstractText": "A new account of parameter setting during grammatical acquisition is presented in terms of Generalized Categorial Grammar embedded in a default inheritance hierarchy, providing a natural partial ordering on the setting of parameters. Experiments show that several experhnentally effective learners can be defined in this framework. Evolutionary simulations suggest that a learner with default initial settings for parameters will emerge, provided that learning is memory limited and the environment of linguistic adaptation contains an appropriate language. ",
      "year": "1997",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 13.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.42073632870420447
        },
        {
          "topicName": "Rule-based parsing",
          "strength": 0.08062671226300089
        },
        {
          "topicName": "Combinatory categorial grammar",
          "strength": 0.052493982650769655
        },
        {
          "topicName": "Numerical expressions and world language data",
          "strength": 0.0475305795989162
        },
        {
          "topicName": "Human sentence processing",
          "strength": 0.04215886478519219
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.041398362622014
        },
        {
          "topicName": "Lexical rules",
          "strength": 0.04050495370366877
        },
        {
          "topicName": "Genetic algorithms & anaphora resolution",
          "strength": 0.03885114000822188
        },
        {
          "topicName": "Parallel corpora for machine translation",
          "strength": 0.026076572932467002
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.02166538924039275
        }
      ],
      "index": 7
    },
    {
      "author": "Clark, Alexander; Lappin, Shalom, ",
      "title": "Another Look at Indirect Negative Evidence",
      "id": "W09-0904",
      "abstractText": "Another look at indirect negative evidence Alexander Clark Shalom Lappin Department of Computer Science Department of Philosophy Royal Holloway, University of London King's College, London alexc@cs.rhul.ac.uk shalom.lappin@kcl.ac.uk Indirect negative evidence is clearly an important way for learners to constrain over-generalisation, and yet a good learning theoretic analysis has yet to be provided for this, whether in a PAC or a probabilistic identification in the limit framework. In this paper we suggest a theoretical analysis of indirect negative evidence that allows the presence of ungrammatical strings in the input and also accounts for the relationship between grammatical-ity/acceptability and probability. Given independently justified assumptions about lower bounds on the probabilities of grammatical strings, we establish that a limited number of membership queries of some strings can be probabilistically simulated. ",
      "year": "2009",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 4.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.4661890934606924
        },
        {
          "topicName": "Language model",
          "strength": 0.09366499415570116
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.07423644431692354
        },
        {
          "topicName": "Finite-state automata",
          "strength": 0.05733498099777728
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.0527191328950879
        },
        {
          "topicName": "Natural languages & computational linguistics",
          "strength": 0.028239417010159244
        },
        {
          "topicName": "Miscellany 2",
          "strength": 0.026749826035727953
        },
        {
          "topicName": "Gibbs sampling",
          "strength": 0.025321392170672734
        },
        {
          "topicName": "Generative models & model selection",
          "strength": 0.023952752164012107
        },
        {
          "topicName": "Supervised learning",
          "strength": 0.02198340310123602
        }
      ],
      "index": 8
    },
    {
      "author": "Mitchener, W. Garrett, ",
      "title": "Simulating Language Change In The Presence Of Non-Idealized Syntax",
      "id": "W05-0502",
      "abstractText": "Both Middle English and Old French had a syntactic property called verb-second or V2 that disappeared. In this paper describes a simulation being developed to shed light on the question of why V2 is stable in some languages, but not others. The simulation, based on a Markov chain, uses fuzzy grammars where speakers can use an arbitrary mixture of idealized grammars. Thus, it can mimic the variable syntax observed in Middle English manuscripts. The simulation supports the hypotheses that children use the topic of a sentence for word order acquisition, that acquisition takes into account the ambiguity of grammatical information available from sample sentences, and that speakers prefer to speak with more regularity than they observe in the primary linguistic data. ",
      "year": "2005",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.38121325592298105
        },
        {
          "topicName": "Hidden Markov models",
          "strength": 0.08906121660005023
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.05852845518040652
        },
        {
          "topicName": "Gibbs sampling",
          "strength": 0.057033151674073336
        },
        {
          "topicName": "Co-reference",
          "strength": 0.05264240689155015
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.047888866003764984
        },
        {
          "topicName": "Language model",
          "strength": 0.03689120907987565
        },
        {
          "topicName": "Rule-based parsing",
          "strength": 0.03337626477841204
        },
        {
          "topicName": "Speaker & hearer",
          "strength": 0.02984456713066301
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.024729276924968787
        }
      ],
      "index": 9
    },
    {
      "author": "Lignos, Constantine; Yang, Charles D., ",
      "title": "Recession Segmentation: Simpler Online Word Segmentation Using Limited Resources",
      "id": "W10-2912",
      "abstractText": "Recession Segmentation: Simpler Online Word Segmentation Using Limited Resources* Constantine Lignos, Charles Yang Dept. of Computer and Information Science, Dept. of Linguistics University of Pennsylvania lignos@cis.upenn.edu, charles.yang@ling.upenn.edu In this paper we present a cognitively plausible approach to word segmentation that segments in an online fashion using only local information and a lexicon of previously segmented words. Unlike popular statistical optimization techniques, the learner uses structural information of the input syllables rather than distributional cues to segment words. We develop a memory model for the learner that like a child learner does not recall previously hypothesized words perfectly. ",
      "year": "2010",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.38057462101521045
        },
        {
          "topicName": "Phonology & syllabification",
          "strength": 0.14039904962582142
        },
        {
          "topicName": "Identifying unknown words",
          "strength": 0.0975488060578844
        },
        {
          "topicName": "Lexical chains & text segmentation",
          "strength": 0.05299494554746031
        },
        {
          "topicName": "Experimental results",
          "strength": 0.046998037748116415
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.04696066706070673
        },
        {
          "topicName": "Training and testing data sets",
          "strength": 0.04189779178131836
        },
        {
          "topicName": "Chinese word segmentation",
          "strength": 0.04023660856204515
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.028764251123895218
        },
        {
          "topicName": "Gibbs sampling",
          "strength": 0.01972961140159086
        }
      ],
      "index": 10
    },
    {
      "author": "Yang, Charles D., ",
      "title": "A Statistical Test for Grammar",
      "id": "W11-0604",
      "abstractText": "A Statistical Test for Grammar Charles Yang Department of Linguistics & Computer Science Institute for Research in Cognitive Science University of Pennsylvania charles.yang@ling.upenn.edu We propose a statistical test for measuring grammatical productivity. We show that very young children's knowledge is consistent with a systematic grammar that independently combines linguistic units. To a testable extent, the usage-based approach to language and language learning, which emphasizes the role of lexically specific memorization, is inconsistent with the child language data. We also discuss the connection of this research with developments in computational and theoretical linguistics. ",
      "year": "2011",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.4237753699619817
        },
        {
          "topicName": "Sampling & Zipf's law",
          "strength": 0.15727206241033667
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.11461815166699804
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.035670151672979336
        },
        {
          "topicName": "Experimental results",
          "strength": 0.03403761000603593
        },
        {
          "topicName": "Morphological analysis",
          "strength": 0.03148352531018907
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.030977644469661458
        },
        {
          "topicName": "Language model",
          "strength": 0.02235916924205438
        },
        {
          "topicName": "Minimum description length",
          "strength": 0.01658807896217569
        },
        {
          "topicName": "Rule-based parsing",
          "strength": 0.01606118298332014
        }
      ],
      "index": 11
    },
    {
      "author": "Pertsova, Katya, ",
      "title": "Comparing Learners for Boolean partitions: Implications for Morphological Paradigms",
      "id": "W09-1010",
      "abstractText": "Comparing learners for Boolean partitions: implications for morphological paradigms * Katya Pertsova University of North Carolina, Chapel Hill pertsova@email.unc.edu In this paper, I show that a problem of learning a morphological paradigm is similar to a problem of learning a partition of the space of Boolean functions. I describe several learners that solve this problem in different ways, and compare their basic properties. ",
      "year": "2009",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.5520699742731523
        },
        {
          "topicName": "Morphological analysis",
          "strength": 0.079616723887694
        },
        {
          "topicName": "Natural languages & computational linguistics",
          "strength": 0.07356884769087899
        },
        {
          "topicName": "Morphological segmentation",
          "strength": 0.04005137908375357
        },
        {
          "topicName": "Feature selection and classifiers in machine learning",
          "strength": 0.03606710654971429
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.030630411236865596
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.030605719477898675
        },
        {
          "topicName": "Objective functions",
          "strength": 0.025383800881219724
        },
        {
          "topicName": "Experimental results",
          "strength": 0.017498841087353563
        },
        {
          "topicName": "Lexical semantics",
          "strength": 0.016224314006828357
        }
      ],
      "index": 12
    },
    {
      "author": "Alishahi, Afra; Fazly, Afsaneh; Stevenson, Suzanne, ",
      "title": "Fast Mapping in Word Learning: What Probabilities Tell Us",
      "id": "W08-2108",
      "abstractText": "Fast Mapping in Word Learning: What Probabilities Tell Us Afra Alishahi and Afsaneh Fazly and Suzanne Stevenson Department of Computer Science University of Toronto {afra,afsaneh,suzanne}@cs.toronto.edu Children can determine the meaning of a new word from hearing it used in a familiar context—an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data. ",
      "year": "2008",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 3.67,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.5153466027961352
        },
        {
          "topicName": "Identifying unknown words",
          "strength": 0.07183947880190573
        },
        {
          "topicName": "Generative models & model selection",
          "strength": 0.07101694374292655
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.054476127191374256
        },
        {
          "topicName": "Training and testing data sets",
          "strength": 0.047701146916110077
        },
        {
          "topicName": "Spatial referring expressions",
          "strength": 0.043217020279325676
        },
        {
          "topicName": "Experimental results",
          "strength": 0.03707213028335711
        },
        {
          "topicName": "Language model",
          "strength": 0.030562601679284724
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.029482615386695134
        },
        {
          "topicName": "Word alignment",
          "strength": 0.017455028731367814
        }
      ],
      "index": 13
    },
    {
      "author": "Villavicencio, Aline, ",
      "title": "The Acquisition Of Word Order By A Computational Learning System",
      "id": "W00-0743",
      "abstractText": "The purpose of this work is to investigate the process of grammatical acquisition from data. We are using a computational learning system that is composed of a Universal Grammar with associated parameters, and a learning algorithm, following the Principles and Parameters Theory. The Universal Grammar is implemented as a Unification-Based Generalised Cat-egorial Grammar, embedded in a default inheritance network of lexical types. The learning algorithm receives input from a corpus annotated with logical forms and sets the parameters based on this input. This framework is used as basis to investigate several aspects of language acquisition. In this paper we are concentrating on the acquisition of word order for different learners. The results obtained show the different learners having a similar performance and converging towards the target grammar given the input data available, regardless of their starting points. It also shows how the amount of noise present in the input data affects the speed of convergence of the learners towards the target. ",
      "year": "2000",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 2.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.47647586080278753
        },
        {
          "topicName": "Lexical rules",
          "strength": 0.08180477182633711
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.05055107353825456
        },
        {
          "topicName": "Combinatory categorial grammar",
          "strength": 0.047813903276381665
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.04696775774520582
        },
        {
          "topicName": "Experimental results",
          "strength": 0.039582571127186134
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.031006502205957815
        },
        {
          "topicName": "Training and testing data sets",
          "strength": 0.028859539528160682
        },
        {
          "topicName": "Language model",
          "strength": 0.025017122835553803
        },
        {
          "topicName": "Feature structures",
          "strength": 0.02445042851938708
        }
      ],
      "index": 14
    },
    {
      "author": "Matusevych, Yevgen; Alishahi, Afra; Backus, Ad, ",
      "title": "Computational simulations of second language construction learning",
      "id": "W13-2606",
      "abstractText": "There are few computational models of second  language acquisition (SLA). At the same time, many questions in the field of SLA remain  unanswered. In particular, SLA patterns are difficult to study due to the large amount of variation between human learners. We present a computational model of second language construction learning that allows manipulating specific parameters such as age of onset and amount of exposure. We use the model to study general developmental patterns of SLA and two specific effects sometimes found in empirical studies: construction priming and a facilitatory effect of skewed frequencies in the input. Our simulations replicate the expected SLA patterns as well as the two effects. Our model can be used in further studies of various SLA phenomena. ",
      "year": "2013",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 0.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.38067074495097725
        },
        {
          "topicName": "Verb classes",
          "strength": 0.12228854507409681
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.07394091159878931
        },
        {
          "topicName": "Natural language & lexical semantics",
          "strength": 0.04335376769531785
        },
        {
          "topicName": "Human sentence processing",
          "strength": 0.035868767483561464
        },
        {
          "topicName": "Training and testing data sets",
          "strength": 0.028256835223347623
        },
        {
          "topicName": "Information extraction patterns",
          "strength": 0.027731367938249495
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.026152397879316558
        },
        {
          "topicName": "Semantic role labeling",
          "strength": 0.025689280679404538
        },
        {
          "topicName": "Experimental results",
          "strength": 0.025480188395570657
        }
      ],
      "index": 15
    },
    {
      "author": "Wilkens, Rodrigo; Villavicencio, Aline, ",
      "title": "I say have you say tem: profiling verbs in children data in English and Portuguese",
      "id": "W12-0910",
      "abstractText": "In this paper we present a profile of verb usage  across ages in child-produced sentences in English and Portuguese. We examine in particular lexical and syntactic characteristics  of verbs and find common trends in these languages as children?s ages increase, such as the prominence of general and poly-semic verbs, as well as divergences such as the proportion of subject dropping. We also find a correlation between the age of acquisition  and the number of complements of a  verb for English. ",
      "year": "2012",
      "pedagogicalRole": null,
      "pageRankScore": 0.0,
      "relevanceScore": 0.0,
      "authorScore": 1.0,
      "relevantTopics": [
        {
          "topicName": "Language acquisition",
          "strength": 0.36023861171595445
        },
        {
          "topicName": "Verb classes",
          "strength": 0.16360330283799085
        },
        {
          "topicName": "Computational linguistics",
          "strength": 0.12785597102833285
        },
        {
          "topicName": "Parallel corpora for machine translation",
          "strength": 0.06331001945779385
        },
        {
          "topicName": "Noun phrases",
          "strength": 0.05635905459582074
        },
        {
          "topicName": "Gold standard",
          "strength": 0.04816802181756711
        },
        {
          "topicName": "Experimental results",
          "strength": 0.04567341995968043
        },
        {
          "topicName": "Annotation",
          "strength": 0.02715263149837978
        },
        {
          "topicName": "Proceedings of the ACL",
          "strength": 0.01707547337141899
        },
        {
          "topicName": "Human assessment",
          "strength": 0.015118749715778682
        }
      ],
      "index": 16
    }
  ]
}
