{
  "keyword": "language_learning",
  "matchTopics": [
    {
      "topic": [
        {
          "word": "language_learning",
          "value": 0.10653138905763626
        },
        {
          "word": "computer_science",
          "value": 0.08941027522087097
        },
        {
          "word": "tutoring_system",
          "value": 0.0824350044131279
        },
        {
          "word": "intelligent_tutoring",
          "value": 0.06721623241901398
        },
        {
          "word": "foreign_language",
          "value": 0.05770450085401535
        },
        {
          "word": "text_based",
          "value": 0.05707038566470146
        },
        {
          "word": "tutoring_systems",
          "value": 0.05199746415019035
        },
        {
          "word": "tutorial_dialogue",
          "value": 0.04438807815313339
        },
        {
          "word": "human_human",
          "value": 0.03994927182793617
        },
        {
          "word": "multiple_choice",
          "value": 0.039315156638622284
        },
        {
          "word": "student_learning",
          "value": 0.03804692625999451
        },
        {
          "word": "student_turns",
          "value": 0.03677869215607643
        },
        {
          "word": "acoustic_prosodic",
          "value": 0.034876346588134766
        },
        {
          "word": "human_tutoring",
          "value": 0.034876346588134766
        },
        {
          "word": "computer_assisted",
          "value": 0.03424223139882088
        },
        {
          "word": "student_state",
          "value": 0.0329740010201931
        },
        {
          "word": "language_technology",
          "value": 0.0329740010201931
        },
        {
          "word": "tutoring_dialogues",
          "value": 0.03170577064156532
        },
        {
          "word": "learning_environment",
          "value": 0.02916930802166462
        },
        {
          "word": "student_turn",
          "value": 0.02916930802166462
        },
        {
          "word": "student_model",
          "value": 0.02916930802166462
        }
      ],
      "documents": [
        {
          "author": "Litman, Diane J.; Silliman, Scott",
          "title": "ITSPOKE: An Intelligent Tutoring Spoken Dialogue System",
          "id": "N04-3002",
          "abstractText": "ITSPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its “back-end”. A student first types a natural language answer to a qualitative physics problem. ITSPOKE then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors. ",
          "year": "2004",
          "pedagogicalRole": null,
          "pageRankScore": 1.3335214E-4,
          "relevanceScore": 0.6814072025394979,
          "authorScore": 10.5
        },
        {
          "author": "Litman, Diane J.; Forbes-Riley, Kate",
          "title": "Annotating Student Emotional States In Spoken Tutoring Dialogues",
          "id": "W04-2326",
          "abstractText": "We present an annotation scheme for student emotions in tutoring dialogues. Analyses of our scheme with respect to interannotator agreement and predictive accuracy indicate that our scheme is reliable in our domain, and that our emotion labels can be predicted with a high degree of accuracy. We discuss issues concerning the implementation of emotion prediction and adaptation in the computer tutoring dialogue system we are developing. ",
          "year": "2004",
          "pedagogicalRole": null,
          "pageRankScore": 7.729904E-5,
          "relevanceScore": 0.5446322119435356,
          "authorScore": 10.5
        },
        {
          "author": "Forbes-Riley, Kate; Litman, Diane J.",
          "title": "Predicting Emotion In Spoken Dialogue From Multiple Knowledge Sources",
          "id": "N04-1026",
          "abstractText": "We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues. We first annotate student turns in our corpus for negative, neutral and positive emotions. We then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions. We compare the results of machine learning experiments using different feature sets to predict the annotated emotions. Our best performing feature set contains both acoustic-prosodic and other types of linguistic features, extracted from both the current turn and a context of previous student turns, and yields a prediction accuracy of 84.75%, which is a 44% relative improvement in error reduction over a baseline. Our results suggest that the intelligent tutoring spoken dialogue system we are developing can be enhanced to automatically predict and adapt to student emotions. ",
          "year": "2004",
          "pedagogicalRole": null,
          "pageRankScore": 7.330861E-5,
          "relevanceScore": 0.456329616344039,
          "authorScore": 10.5
        }
      ],
      "dependentTopics": [
        {
          "topic": [
            {
              "word": "natural_language",
              "value": 0.20899774134159088
            },
            {
              "word": "point_view",
              "value": 0.0812462642788887
            },
            {
              "word": "computational_linguistics",
              "value": 0.07637516409158707
            },
            {
              "word": "syntactic_semantic",
              "value": 0.06833325326442719
            },
            {
              "word": "language_processing",
              "value": 0.06364597380161285
            },
            {
              "word": "semantic_representation",
              "value": 0.05077891796827316
            },
            {
              "word": "lexical_items",
              "value": 0.04641330987215042
            },
            {
              "word": "syntactic_structure",
              "value": 0.0393364280462265
            },
            {
              "word": "abstract_paper",
              "value": 0.03625752404332161
            },
            {
              "word": "syntax_semantics",
              "value": 0.03414365276694298
            },
            {
              "word": "semantic_structure",
              "value": 0.032489314675331116
            },
            {
              "word": "natural_languages",
              "value": 0.0294563677161932
            },
            {
              "word": "previous_section",
              "value": 0.02734249271452427
            },
            {
              "word": "semantic_analysis",
              "value": 0.026928909122943878
            },
            {
              "word": "semantic_information",
              "value": 0.02633151039481163
            },
            {
              "word": "figure_shows",
              "value": 0.02591792680323124
            },
            {
              "word": "linguistics_volume",
              "value": 0.025826018303632736
            },
            {
              "word": "linguistic_phenomena",
              "value": 0.02578006498515606
            },
            {
              "word": "takes_place",
              "value": 0.02522861957550049
            },
            {
              "word": "starting_point",
              "value": 0.024998851120471954
            },
            {
              "word": "discussed_section",
              "value": 0.024171683937311172
            }
          ],
          "documents": [
            {
              "author": "Calzolari, Nicoletta",
              "title": "Towards The Organization Of Lexical Definitions On A Database Structure",
              "id": "C82-2013",
              "abstractText": "Printed dictionaries are great repositories of information, and it is important that they can be exploited as fully as possible, with regard to all the different types of data they contain. This was one of the aims when organizing the Machine Dictionary of the Italian language on a database structure. The design and organization of the lexical database for the first two relations implemented, i.e. the set of Lemmas (106, 091) and the set of Word-forms (1,016,320), has been described in other papers (see for example Calzolari and Cec* cotti, 1980). These two very large archives are maintained continuously on-line and are interactively invoked through a query language which permits to the user to access, in transparent mode, the data, and to have his particular \"view\" of the data. The database concept and methodology give rise, in fact, to a radical change in perspective when confronted with sequential organization of data. We have a dynamic rather than a static object which is flexible and easy to query, update, extend. This lexical database is now being extended by the insertion of lexical definitions (185,899) and semantic data. The guiding principle behind this project is the conviction that the study of the defining vocabulary of an actual dictionary can provide a precious tool in the semantic analysis - 61 - of a language (see Noel, 1981). The logical organization of this definitional information is not a trivial task, and must be performed bearing in utnd the goals to be achieved. It must in fact be possible to have direct access to each and every piece of information contained in the definitions. ",
              "year": "1982",
              "pedagogicalRole": null,
              "pageRankScore": 4.741768E-4,
              "relevanceScore": 0.23150763717175304,
              "authorScore": 3.0
            },
            {
              "author": "Winograd, Terry",
              "title": "On Primitives, Prototypes, And Other Semantic Anomalies",
              "id": "T78-1004",
              "abstractText": "Over the past few years, there have been a number of papers arguing the relative merits of primitives and prototypes as representations for the meaning of natural language. Much of the discussion has been both pugnacious and confused, with each author setting up one or another straw-man to knock down. Much of the confusion has resulted from a lack of agreement as to what it would mean for a system to use primitives or prototypes. There are several different dimensions along which semantic formalisms vary, and many of the arguments have blurred these into a single distinction. In this paper, I propose a framework within which to compare a variety of semantic formalisms which have been proposed in linguistics and artificial intelligence. The paper lays out three dimensions (called ontological, logical, and relational), describing the relevant options along each and the implications of making alternative choices in the design of a formalism. It does not attempt to demonstrate that one or another alternative is right, but instead tries to clearly state the advantages and disadvantages of each in a non-partisan way. It is more in the style of a textbook than of a research paper. Its contribution will, I hope, be in dissolving some non-issues which have occupied previous discussion, and in focussing attention on the real distinctions between alternative proposals. My own prejudices are set forth in Winograd (1976) and Bobrow and Winograd (1977). ",
              "year": "1978",
              "pedagogicalRole": null,
              "pageRankScore": 3.6425097E-4,
              "relevanceScore": 0.2800230555381128,
              "authorScore": 0.0
            },
            {
              "author": "Kaplan, Ronald M.; Netter, Klaus; Wedekind, Jurgen; Zaenen, Annie",
              "title": "Translation By Structural Correspondences",
              "id": "E89-1037",
              "abstractText": "We sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation, as formalized in the LFG notion of codescriptions. The approach is illustrated with examples from English, German and French where the source and the target language sentence show noteworthy differences in linguistic analysis. ",
              "year": "1989",
              "pedagogicalRole": null,
              "pageRankScore": 2.5290402E-4,
              "relevanceScore": 0.24997576232776936,
              "authorScore": 3.5
            }
          ],
          "dependentTopics": []
        },
        {
          "topic": [
            {
              "word": "information_extraction",
              "value": 0.08652849495410919
            },
            {
              "word": "language_processing",
              "value": 0.08134715259075165
            },
            {
              "word": "language_technology",
              "value": 0.078238345682621
            },
            {
              "word": "language_engineering",
              "value": 0.06683937460184097
            },
            {
              "word": "large_scale",
              "value": 0.060103628784418106
            },
            {
              "word": "open_source",
              "value": 0.04974093288183212
            },
            {
              "word": "state_art",
              "value": 0.048704661428928375
            },
            {
              "word": "human_language",
              "value": 0.04663212597370148
            },
            {
              "word": "research_development",
              "value": 0.045077718794345856
            },
            {
              "word": "development_environment",
              "value": 0.04300517961382866
            },
            {
              "word": "tipster_architecture",
              "value": 0.042487047612667084
            },
            {
              "word": "speech_technology",
              "value": 0.041450776159763336
            },
            {
              "word": "phase_ii",
              "value": 0.03989637270569801
            },
            {
              "word": "verification_method",
              "value": 0.03834196925163269
            },
            {
              "word": "data_structures",
              "value": 0.03575129434466362
            },
            {
              "word": "technology_transfer",
              "value": 0.03316062316298485
            },
            {
              "word": "cunningham_al",
              "value": 0.03316062316298485
            },
            {
              "word": "language_technologies",
              "value": 0.032642487436532974
            },
            {
              "word": "tipster_phase",
              "value": 0.032642487436532974
            },
            {
              "word": "tipster_text",
              "value": 0.032642487436532974
            },
            {
              "word": "tipster_program",
              "value": 0.03160621598362923
            }
          ],
          "documents": [
            {
              "author": "Cunningham, Hamish; Maynard, Diana; Bontcheva, Kalina; Tablan, Valentin",
              "title": "GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications",
              "id": "P02-1022",
              "abstractText": "In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion. The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated. The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support. ",
              "year": "2002",
              "pedagogicalRole": null,
              "pageRankScore": 6.822444E-4,
              "relevanceScore": 0.4695247156156728,
              "authorScore": 2.5
            },
            {
              "author": "Loper, Edward; Bird, Steven",
              "title": "NLTK: The Natural Language Toolkit",
              "id": "W02-0109",
              "abstractText": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. ",
              "year": "2002",
              "pedagogicalRole": null,
              "pageRankScore": 3.466649E-4,
              "relevanceScore": 0.406544186853387,
              "authorScore": 2.5
            },
            {
              "author": "Bontcheva, Kalina; Cunningham, Hamish; Tablan, Valentin; Maynard, Diana; Hamza, Oana",
              "title": "Using GATE As An Environment For Teaching NLP",
              "id": "W02-0108",
              "abstractText": "In this paper we argue that the GATE architecture and visual development environment can be used as an effective tool for teaching language engineering and computational linguistics. Since GATE comes with a customisable and extendable set of components, it allows students to get hands-on experience with building NLP applications. GATE also has tools for corpus annotation and performance evaluation, so students can go through the entire application development process within its graphical development environment. Finally, it offers comprehensive Unicode-compliant multilingual support, thus allowing students to create components for languages other than English. Unlike other NLP teaching tools which were designed specifically and only for this purpose, GATE is a system developed for and used actively in language engineering research. This unique duality allows students to contribute to research projects and gain skills in embedding HLT in practical applications. ",
              "year": "2002",
              "pedagogicalRole": null,
              "pageRankScore": 3.118785E-4,
              "relevanceScore": 0.521648507001222,
              "authorScore": 2.0
            }
          ],
          "dependentTopics": []
        },
        {
          "topic": [
            {
              "word": "natural_language",
              "value": 0.21139150857925415
            },
            {
              "word": "knowledge_base",
              "value": 0.20002569258213043
            },
            {
              "word": "knowledge_representation",
              "value": 0.08232196420431137
            },
            {
              "word": "language_understanding",
              "value": 0.05779233202338219
            },
            {
              "word": "world_knowledge",
              "value": 0.04860977455973625
            },
            {
              "word": "domain_knowledge",
              "value": 0.03666602447628975
            },
            {
              "word": "artificial_intelligence",
              "value": 0.03133628889918327
            },
            {
              "word": "data_base",
              "value": 0.030180441215634346
            },
            {
              "word": "shown_figure",
              "value": 0.02979515865445137
            },
            {
              "word": "knowledge_sources",
              "value": 0.028318243101239204
            },
            {
              "word": "knowledge_based",
              "value": 0.026327617466449738
            },
            {
              "word": "linguistic_knowledge",
              "value": 0.024529634043574333
            },
            {
              "word": "understanding_system",
              "value": 0.024401206523180008
            },
            {
              "word": "semantic_network",
              "value": 0.023309573531150818
            },
            {
              "word": "real_world",
              "value": 0.023181146010756493
            },
            {
              "word": "language_processing",
              "value": 0.023181146010756493
            },
            {
              "word": "problem_solving",
              "value": 0.023052720353007317
            },
            {
              "word": "expert_system",
              "value": 0.019585179165005684
            },
            {
              "word": "common_sense",
              "value": 0.019520966336131096
            },
            {
              "word": "knowledge_bases",
              "value": 0.019520966336131096
            },
            {
              "word": "system_knowledge",
              "value": 0.01695241779088974
            }
          ],
          "documents": [
            {
              "author": "Woods, William A.",
              "title": "Taxonomic Lattice Structures For Situation Recognition",
              "id": "T78-1005",
              "abstractText": "The kinds of intelligent computer assistants that we would like to be able to construct are very much like intelligent organisms in their own right. Imagine for a moment an intelligent organism trying to get along in the world (find enough food, stay out of trouble, satisfy basic needs, etc.). The most valuable service played by an internal knowledge base for such an organism is to repeatedly answer questions like \"what's going on out there?\", \"can it harm me?\", \"how can I avoid/placate it?\", \"Is it good to eat?\", \"Is there any special thing I should do about it?\", etc. To support this kind of activity, a substantial part of the knowledge base must be organized as a recognition device for classifying and identifying situations in the world. The major purpose of this situation recognition is to locate internal procedures which are applicable (appropriate, permitted, mandatory, etc.) to the current situation. In constructing an intelligent computer assistant, the roles of knowledge are very similar. The basic goals of food getting and danger avoidance are replaced by goals of doing what the user wants and avoiding things that the machine has been instructed to avoid. However, the fundamental problem of analyzing a situation (one established either linguistically or physically or by some combination of the two) in order to determine whether it is one for which there are procedures to be executed, or one which was to be avoided (or one which might lead to one that is to be avoided), etc. is basically the same. ",
              "year": "1978",
              "pedagogicalRole": null,
              "pageRankScore": 1.7742722E-4,
              "relevanceScore": 0.20782189971315804,
              "authorScore": 0.0
            },
            {
              "author": "Wilensky, Robert; Arens, Yigal",
              "title": "PHRAN - A Knowledge-Based Natural Language Understander",
              "id": "P80-1030",
              "abstractText": "We have developed an approach to natural language processing in which the natural language processor is viewed as a knowledge-based system whose knowledge is about the meanings of the utterances of its language. The approach is oriented around the phrase rather than the word as the basic unit. We believe that this paradigm for language processing not only extends the capabilities of other natural language systems, but handles those tasks that previous systems could perform in a more systematic and extensible manner. We have constructed a natural language analysis program called PHRAN (PHRasal ANalyzer) based in this approach. This model has a number of advantages over existing systems, including the ability to understand a wider variety of language utterances, increased processing speed in some cases, a clear separation of control structure from data structure, a knowledge base that could be shared by a language production mechanism, greater ease of extensibility, and the ability to store some useful forms of knowledge that cannot readily be added to other systems. ",
              "year": "1980",
              "pedagogicalRole": null,
              "pageRankScore": 1.6612625E-4,
              "relevanceScore": 0.20497167373692027,
              "authorScore": 1.5
            },
            {
              "author": "Rau, Lisa F.; Jacobs, Paul S.",
              "title": "Integrating Top-Down And Bottom-Up Strategies In A Text Processing System",
              "id": "A88-1018",
              "abstractText": "The SCISOR system is a computer program designed to scan naturally occurring texts in constrained domains, extract information, and answer questions about that information. The system currently reads newspapers stories in the domain of corporate mergers and acquisitions. The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing. Four knowledge sources, including syntactic and semantic information and domain knowledge, interact in a flexible manner. This integration produces a more robust semantic analyzer designed to deal gracefully with gaps in lexical and syntactic knowledge, transports easily to new domains, and facilitates the extraction of information from texts. ",
              "year": "1988",
              "pedagogicalRole": null,
              "pageRankScore": 1.3227452E-4,
              "relevanceScore": 0.26548333391646095,
              "authorScore": 2.0
            }
          ],
          "dependentTopics": []
        }
      ]
    },
    {
      "topic": [
        {
          "word": "language_acquisition",
          "value": 0.19373367726802826
        },
        {
          "word": "language_learning",
          "value": 0.1060052216053009
        },
        {
          "word": "learning_algorithm",
          "value": 0.073107048869133
        },
        {
          "word": "construction_grammar",
          "value": 0.05378590151667595
        },
        {
          "word": "learning_process",
          "value": 0.05117493495345116
        },
        {
          "word": "negative_evidence",
          "value": 0.04908616095781326
        },
        {
          "word": "learning_system",
          "value": 0.04699739068746567
        },
        {
          "word": "child_language",
          "value": 0.04595300182700157
        },
        {
          "word": "item_based",
          "value": 0.03968668356537819
        },
        {
          "word": "form_meaning",
          "value": 0.0355091392993927
        },
        {
          "word": "learning_language",
          "value": 0.033942557871341705
        },
        {
          "word": "parameter_setting",
          "value": 0.03185378760099411
        },
        {
          "word": "word_order",
          "value": 0.03185378760099411
        },
        {
          "word": "language_learner",
          "value": 0.02924281917512417
        },
        {
          "word": "grammar_learning",
          "value": 0.02924281917512417
        },
        {
          "word": "universal_grammar",
          "value": 0.02819843403995037
        },
        {
          "word": "parameter_values",
          "value": 0.02558746747672558
        },
        {
          "word": "target_grammar",
          "value": 0.02506527490913868
        },
        {
          "word": "mental_spaces",
          "value": 0.02454308047890663
        },
        {
          "word": "positive_evidence",
          "value": 0.022976500913500786
        },
        {
          "word": "target_language",
          "value": 0.022454308345913887
        }
      ],
      "documents": [
        {
          "author": "Goodman, Joshua",
          "title": "Semiring Parsing",
          "id": "J99-4004",
          "abstractText": "We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation. ",
          "year": "1999",
          "pedagogicalRole": null,
          "pageRankScore": 1.08333734E-4,
          "relevanceScore": 0.42917822035898623,
          "authorScore": 7.0
        },
        {
          "author": "Yang, Charles D.",
          "title": "A Selectionist Theory Of Language Acquisition",
          "id": "P99-1055",
          "abstractText": "This paper argues that developmental patterns in child language be taken seriously in computational models of language acquisition, and proposes a formal theory that meets this criterion. We first present developmental facts that are problematic for statistical learning approaches which assume no prior knowledge of grammar, and for traditional learnabil-ity models which assume the learner moves from one UG-defined grammar to another. In contrast, we view language acquisition as a population of grammars associated with \"weights\", that compete in a Darwinian selectionist process. Selection is made possible by the variational properties of individual grammars; specifically, their differential compatibility with the primary linguistic data in the environment. In addition to a convergence proof, we present empirical evidence in child language development, that a learner is best modeled as multiple grammars in coexistence and competition. ",
          "year": "1999",
          "pedagogicalRole": null,
          "pageRankScore": 5.2213963E-5,
          "relevanceScore": 0.6371234942061708,
          "authorScore": 0.0
        },
        {
          "author": "Sagae, Kenji; Lavie, Alon; MacWhinney, Brian",
          "title": "Automatic Measurement Of Syntactic Development In Child Language",
          "id": "P05-1025",
          "abstractText": "To facilitate the use of syntactic information in the study of child language acquisition, a coding scheme for Grammatical Relations (GRs) in transcripts of parent-child dialogs has been proposed by Sagae, MacWhinney and Lavie (2004). We discuss the use of current NLP techniques to produce the GRs in this annotation scheme. By using a statistical parser (Charniak, 2000) and memory-based learning tools for classification (Daelemans et al., 2004), we obtain high precision and recall of several GRs. We demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the Index of Productive Syntax (Scarborough, 1990) at similar levels to what child language researchers compute manually. ",
          "year": "2005",
          "pedagogicalRole": null,
          "pageRankScore": 4.7951948E-5,
          "relevanceScore": 0.28779741786224844,
          "authorScore": 4.0
        }
      ],
      "dependentTopics": [
        {
          "topic": [
            {
              "word": "table_shows",
              "value": 0.09514964371919632
            },
            {
              "word": "precision_recall",
              "value": 0.08131016790866852
            },
            {
              "word": "test_set",
              "value": 0.0781969428062439
            },
            {
              "word": "figure_shows",
              "value": 0.0729907900094986
            },
            {
              "word": "future_work",
              "value": 0.06438363343477249
            },
            {
              "word": "shown_table",
              "value": 0.05648283660411835
            },
            {
              "word": "shown_figure",
              "value": 0.04567810893058777
            },
            {
              "word": "natural_language",
              "value": 0.04473629221320152
            },
            {
              "word": "experimental_results",
              "value": 0.04429154470562935
            },
            {
              "word": "data_set",
              "value": 0.04201548919081688
            },
            {
              "word": "previous_work",
              "value": 0.04083821550011635
            },
            {
              "word": "total_number",
              "value": 0.039687108248472214
            },
            {
              "word": "results_show",
              "value": 0.03678317368030548
            },
            {
              "word": "related_work",
              "value": 0.03628610447049141
            },
            {
              "word": "gold_standard",
              "value": 0.03403620794415474
            },
            {
              "word": "test_data",
              "value": 0.03283277526497841
            },
            {
              "word": "state_art",
              "value": 0.032231058925390244
            },
            {
              "word": "based_approach",
              "value": 0.032178737223148346
            },
            {
              "word": "results_obtained",
              "value": 0.031681664288043976
            },
            {
              "word": "section_describes",
              "value": 0.02914399281144142
            },
            {
              "word": "recall_precision",
              "value": 0.029065508395433426
            }
          ],
          "documents": [
            {
              "author": "Koehn, Philipp",
              "title": "Statistical Significance Tests For Machine Translation Evaluation",
              "id": "W04-3250",
              "abstractText": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real. ",
              "year": "2004",
              "pedagogicalRole": null,
              "pageRankScore": 3.7507055E-4,
              "relevanceScore": 0.2751257321769039,
              "authorScore": 12.0
            },
            {
              "author": "Bikel, Daniel M.",
              "title": "Intricacies Of Collins Parsing Model",
              "id": "J04-4004",
              "abstractText": "This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results. Indeed, these as-yet-unpublished details account for an 11 % relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. ",
              "year": "2004",
              "pedagogicalRole": null,
              "pageRankScore": 2.6883994E-4,
              "relevanceScore": 0.23316031320320074,
              "authorScore": 0.0
            },
            {
              "author": "Ostendorf, Mari; Kannan, Ashvin; Austin, Steve; Kimball, Owen; Schwartz, Richard M.; Rohlicek, J. Robin",
              "title": "Integration Of Diverse Recognition Methodologies Through Reevaluation Of N-Best Sentence Hypotheses",
              "id": "H91-1013",
              "abstractText": "This paper describes a general formalism for integrating two or more speech recognition technologies, which could be developed at different research sites using different recognition strategies. In this formalism, one system uses the N-best search strategy to generate a list of candidate sentences; the list is rescored by other systems; and the different scores are combined to optimize performance. Specifically, we report on combining the BU system based on stochastic segment models and the BBN system based on hidden Markov models. In addition to facilitating integration of different systems, the N-best approach results in a large reduction in computation for word recognition using the stochastic segment model. ",
              "year": "1991",
              "pedagogicalRole": null,
              "pageRankScore": 2.5409975E-4,
              "relevanceScore": 0.25326939164583306,
              "authorScore": 7.0
            }
          ],
          "dependentTopics": []
        },
        {
          "topic": [
            {
              "word": "hand_side",
              "value": 0.11037323623895645
            },
            {
              "word": "grammar_rules",
              "value": 0.09767989069223404
            },
            {
              "word": "context_free",
              "value": 0.07641223818063736
            },
            {
              "word": "set_rules",
              "value": 0.07262945175170898
            },
            {
              "word": "phrase_structure",
              "value": 0.06455951929092407
            },
            {
              "word": "left_hand",
              "value": 0.061617352068424225
            },
            {
              "word": "rules_applied",
              "value": 0.0513618029654026
            },
            {
              "word": "rule_set",
              "value": 0.04757901653647423
            },
            {
              "word": "rule_rule",
              "value": 0.04009751230478287
            },
            {
              "word": "grammar_rule",
              "value": 0.03984532505273819
            },
            {
              "word": "rules_rules",
              "value": 0.038836583495140076
            },
            {
              "word": "rule_applied",
              "value": 0.038248151540756226
            },
            {
              "word": "structure_rules",
              "value": 0.03547411039471626
            },
            {
              "word": "number_rules",
              "value": 0.033708810806274414
            },
            {
              "word": "rules_grammar",
              "value": 0.029421653598546982
            },
            {
              "word": "rule_application",
              "value": 0.029253531247377396
            },
            {
              "word": "rules_rule",
              "value": 0.02816072665154934
            },
            {
              "word": "side_rule",
              "value": 0.026563551276922226
            },
            {
              "word": "structure_grammar",
              "value": 0.02639542706310749
            },
            {
              "word": "rule_based",
              "value": 0.02639542706310749
            },
            {
              "word": "rules_apply",
              "value": 0.025386685505509377
            }
          ],
          "documents": [
            {
              "author": "Pereira, Fernando",
              "title": "Extraposition Grammars",
              "id": "J81-4003",
              "abstractText": "This paper presents a grammar formalism for natural language analysis, called extraposition grammars (XGs), based on the subset of predicate calculus known as definite, or Horn, clauses. It is argued that certain important linguistic phenomena, collectively known in transformational grammar as left extraposition, can be described better in XGs than in earlier grammar formalisms based on definite clauses. The XG formalism is an extension of the definite clause grammar (DCG) [6] formalism, which is itself a restriction of Colmerauer's formalism of metamorphosis grammars (MGs) [2]. Thus XGs and MGs may be seen as two alternative extensions of the same basic formalism, DCGs. The argument for XGs will start with a comparison with DCGs. I should point out, however, that the motivation for the development of XGs came from studying large MGs for natural language [4,7]. The relationship between MGs and DCGs is analogous to that between type-0 grammars and context-free grammars. So, some of the linguistic phenomena which are seen as rewriting one sequence of constituents into another might be described better in a MG than in a DCG. However, it will be shown that rewritings such as the one involved in left extraposition cannot easily be described in either of the two formalisms. Left extraposition has been used by grammarians to describe the form of interrogative sentences and relative clauses, at least in languages such as English, French, Spanish and Portuguese. ",
              "year": "1981",
              "pedagogicalRole": null,
              "pageRankScore": 0.0017935759,
              "relevanceScore": 0.3069505611745196,
              "authorScore": 6.0
            },
            {
              "author": "Thompson, Henry S.",
              "title": "Chart Parsing And Rule Schemata In PSG",
              "id": "P81-1036",
              "abstractText": "The advantage of the WFST comes out if we suppose the grammar involved recognises the structural ambiguity of this sentence. If the parsing continued in order to produce the other structure, with the PP attached at the VP level, considerable effort would be saved by the WFST. The subject NP and the PP itself would not need to be reparsed, as they are already in the graph. MCHART is a flexible, modular chart parsing framework I have been developing (in Lisp) at Edinburgh, whose initial design characteristics were largely determined by pedagogical needs. PSG is a grammatical theory developed by Gerald Gazdar at Sussex, in collaboration with others in both the US and Britain, most notably Ivan Sag, Geoff Pullum, and Ewan Klein. It is a notationally rich context free phrase structure grammar, incorporating meta-rules and rule schemata to capture generalisations. (Gazdar 1980a, 1980b, 1981; Gazdar & Sag 1980; Gazdar, Sag, Pullum & Klein to appear) In this paper I want to describe how I have used MCHART in beginning to construct a parser for grammars expressed in PSG, and how aspects of the chart parsing approach in general and MCHART in particular have made it easy to accommodate two significant aspects of PSG: rule schemata involving variables over categories; and compound category symbols (\"slash\" categories). To do this I will briefly introduce the basic ideas of chart parsing; describe the salient aspects of MCHART; give an overview of PSG; and finally present the interesting aspects of the parser I am building for PSG using MCHART. Limitations of space, time, and will mean that all of these sections will be brief and sketchy - I hope to produce a much expanded version at a later date. I. ",
              "year": "1981",
              "pedagogicalRole": null,
              "pageRankScore": 6.671315E-4,
              "relevanceScore": 0.2250240595689901,
              "authorScore": 3.0
            },
            {
              "author": "Chester, Daniel",
              "title": "A Parsing Algorithm That Extends Phrases",
              "id": "J80-2002",
              "abstractText": "It is desirable for a parser to be able to extend a phrase even after it has been combined into a larger syntactic unit. This paper presents an algorithm that does this in two ways, one dealing with \"right extension\" and the other with \"left recursion\". A brief comparison with other parsing algorithms shows it to be related to the left-corner parsing algorithm, but it is more flexible in the order that it permits phrases to be combined. It has many of the properties of the sentence analyzers of Marcus and Riesbeck, but is independent of the language theories on which those programs are based. ",
              "year": "1980",
              "pedagogicalRole": null,
              "pageRankScore": 4.4641405E-4,
              "relevanceScore": 0.26005284443489873,
              "authorScore": 0.0
            }
          ],
          "dependentTopics": []
        },
        {
          "topic": [
            {
              "word": "context_free",
              "value": 0.28259485960006714
            },
            {
              "word": "free_grammars",
              "value": 0.06783972680568695
            },
            {
              "word": "free_grammar",
              "value": 0.06243374943733215
            },
            {
              "word": "input_string",
              "value": 0.0466398149728775
            },
            {
              "word": "parsing_algorithm",
              "value": 0.04314182698726654
            },
            {
              "word": "terminal_symbols",
              "value": 0.04197583347558975
            },
            {
              "word": "polynomial_time",
              "value": 0.04176383465528488
            },
            {
              "word": "hand_side",
              "value": 0.04112783446907997
            },
            {
              "word": "worst_case",
              "value": 0.04049183800816536
            },
            {
              "word": "time_complexity",
              "value": 0.03752385079860687
            },
            {
              "word": "finite_set",
              "value": 0.033283866941928864
            },
            {
              "word": "normal_form",
              "value": 0.03137587383389473
            },
            {
              "word": "lr_parsing",
              "value": 0.030315877869725227
            },
            {
              "word": "left_corner",
              "value": 0.02936188317835331
            },
            {
              "word": "parsing_algorithms",
              "value": 0.028831884264945984
            },
            {
              "word": "start_symbol",
              "value": 0.026605892926454544
            },
            {
              "word": "context_sensitive",
              "value": 0.02490990050137043
            },
            {
              "word": "parse_forest",
              "value": 0.024803901091217995
            },
            {
              "word": "free_languages",
              "value": 0.02310790680348873
            },
            {
              "word": "earley_algorithm",
              "value": 0.02109391614794731
            },
            {
              "word": "natural_languages",
              "value": 0.020775916054844856
            }
          ],
          "documents": [
            {
              "author": "Pereira, Fernando; Schabes, Yves",
              "title": "Inside-Outside Reestimation From Partially Bracketed Corpora",
              "id": "P92-1017",
              "abstractText": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of hand-parsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided. ",
              "year": "1992",
              "pedagogicalRole": null,
              "pageRankScore": 0.0021685073,
              "relevanceScore": 0.6736578604099989,
              "authorScore": 9.5
            },
            {
              "author": "Tomita, Masaru",
              "title": "An Efficient Augmented-Context-Free Parsing Algorithm",
              "id": "J87-1004",
              "abstractText": "An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a \"graph-structured stack\". The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a five to tenfold speed advantage over Earley's context-free parsing algorithm. The algorithm parses a sentence strictly from left to right on-line, that is, it starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP AI workstations. The parser is used in the multilingual machine translation project at CMU. ",
              "year": "1987",
              "pedagogicalRole": null,
              "pageRankScore": 4.926704E-4,
              "relevanceScore": 0.45513797096758407,
              "authorScore": 2.0
            },
            {
              "author": "Eisner, Jason M.; Satta, Giorgio",
              "title": "Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars",
              "id": "P99-1059",
              "abstractText": "Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present 0(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of 0(n5). For a common special case that was known to allow 0(n3) parsing (Eisner, 1997), we present an 0(n3) algorithm with an improved grammar constant. ",
              "year": "1999",
              "pedagogicalRole": null,
              "pageRankScore": 2.979491E-4,
              "relevanceScore": 0.558038938760888,
              "authorScore": 11.0
            }
          ],
          "dependentTopics": []
        }
      ]
    }
  ]
}
